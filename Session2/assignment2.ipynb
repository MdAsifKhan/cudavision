{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we implement a simple single hidden layer neural networks with logistic regression classifier on CIFAR10 dataset. \n",
    "Our implementation consists of 2 main class:\n",
    "\n",
    "* LogisticRegression \n",
    "\n",
    "    Contains implementation of logistic regression classifier with 1 hidden layer on top of pytorch nn Module.\n",
    "\n",
    "\n",
    "* ModelEvaluator \n",
    "\n",
    "    Class consisting of basic functionalities for training, testing and visualizing loss.\n",
    "\n",
    "For extra task we implement another class named: CrossValidation. THis class supports functionality of tuning hyperparameters with grid search and k-fold cross validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing basic stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "import numpy as np\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement logistic regression classifier using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        '''\n",
    "        n_in: Number of Inputs\n",
    "        n_hidden: Number of Hidden Units\n",
    "        n_out: Number of Output Units\n",
    "        '''\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.n_hidden = n_hidden\n",
    "        self.fc1 = nn.Linear(self.n_in, self.n_hidden)\n",
    "        self.fc2 = nn.Linear(self.n_hidden, self.n_out)\n",
    "        self.nonlin = nn.ReLU()\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        forward pass\n",
    "        '''\n",
    "        return self.fc2(self.nonlin(self.fc1(X)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic functionalities for evaluating model is implemented as a part of ModelEvaluator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model, epochs, lr, use_gpu=False, optim='adam'):\n",
    "        '''\n",
    "        model: instance of pytorch model class\n",
    "        epochs: number of training epochs\n",
    "        lr: learning rate\n",
    "        use_gpu: to use gpu\n",
    "        optim: optimizer used for training, SGD or adam\n",
    "        '''\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.epoch_loss = []\n",
    "        if self.use_gpu:\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "            if self.device == 'cuda:0':\n",
    "                if torch.cuda.device_count()>1:\n",
    "                    self.model = nn.DataParallel(model)\n",
    "                self.model.to(device)\n",
    "        if optim=='adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        elif optim=='sgd':\n",
    "            self.optimizer = optim.SGD(self.model.parameters(), lr = lr, momentum=0.9)\n",
    "        else:\n",
    "            ValueError('Optimizer Not Supported')\n",
    "\n",
    "\n",
    "    def train(self, trainloader, testloader, validation=False):\n",
    "        '''\n",
    "        method for training\n",
    "        '''\n",
    "        iter_ = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('Epoch-{}'.format(epoch+1))\n",
    "            print('-----------------')\n",
    "            loss_batch = []\n",
    "            for train_data, train_labels in trainloader:\n",
    "                if self.use_gpu and self.device == 'cuda:0':\n",
    "                    train_data, train_labels = train_data.to(self.device), train_labels.to(self.device)\n",
    "                train_data = train_data.reshape(-1, 32*32*3)\n",
    "                train_data = train_data / 255\n",
    "                train_preds = self.model.forward(train_data)\n",
    "                loss = self.model.loss(train_preds, train_labels)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                iter_ += 1\n",
    "                print('Iter-{0}, training loss{1:.2f}'.format(iter_, loss))\n",
    "                if validation:\n",
    "                    if iter_%500 == 0:\n",
    "                        acc_test = self.test(testloader)\n",
    "                        print('Accuracy on Test Set {:.2f}'.format(acc_test))\n",
    "                loss_batch.append(loss)\n",
    "            self.epoch_loss.append(np.sum(loss_batch))    \n",
    "\n",
    "    def test(self, testloader):\n",
    "        '''\n",
    "        method for testing\n",
    "        '''\n",
    "        correct_ = 0\n",
    "        total_ = 0\n",
    "        with torch.no_grad():\n",
    "            for test_data, test_labels in testloader:\n",
    "                if self.use_gpu and self.device == 'cuda:0':\n",
    "                    test_data, test_labels = test_data.to(self.device), test_labels.to(self.device)\n",
    "                test_data = test_data.reshape(-1, 32*32*3)\n",
    "                test_data = test_data / 255\n",
    "                test_preds = self.model.forward(test_data)\n",
    "                _, test_pred_labels = torch.max(test_preds.data, 1)\n",
    "                total_ += test_labels.size(0)\n",
    "                correct_ += (test_pred_labels.cpu() == test_labels.cpu()).sum()\n",
    "                accuracy_test = (100*correct_/total_)\n",
    "            return accuracy_test\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        to visualize loss\n",
    "        '''\n",
    "        plt.plot(range(len(self.epoch_loss)), self.epoch_loss)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = dsets.CIFAR10('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = dsets.CIFAR10('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_in = np.prod(trainset[0][0].numpy().shape)\n",
    "n_out = len(classes)\n",
    "batch_size = 100\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "n_hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1\n",
      "-----------------\n",
      "Iter-1, training loss2.30\n",
      "Iter-2, training loss2.30\n",
      "Iter-3, training loss2.31\n",
      "Iter-4, training loss2.30\n",
      "Iter-5, training loss2.30\n",
      "Iter-6, training loss2.30\n",
      "Iter-7, training loss2.30\n",
      "Iter-8, training loss2.30\n",
      "Iter-9, training loss2.30\n",
      "Iter-10, training loss2.30\n",
      "Iter-11, training loss2.30\n",
      "Iter-12, training loss2.30\n",
      "Iter-13, training loss2.30\n",
      "Iter-14, training loss2.29\n",
      "Iter-15, training loss2.30\n",
      "Iter-16, training loss2.30\n",
      "Iter-17, training loss2.30\n",
      "Iter-18, training loss2.29\n",
      "Iter-19, training loss2.31\n",
      "Iter-20, training loss2.29\n",
      "Iter-21, training loss2.29\n",
      "Iter-22, training loss2.30\n",
      "Iter-23, training loss2.30\n",
      "Iter-24, training loss2.30\n",
      "Iter-25, training loss2.30\n",
      "Iter-26, training loss2.30\n",
      "Iter-27, training loss2.29\n",
      "Iter-28, training loss2.29\n",
      "Iter-29, training loss2.28\n",
      "Iter-30, training loss2.29\n",
      "Iter-31, training loss2.29\n",
      "Iter-32, training loss2.30\n",
      "Iter-33, training loss2.30\n",
      "Iter-34, training loss2.29\n",
      "Iter-35, training loss2.28\n",
      "Iter-36, training loss2.29\n",
      "Iter-37, training loss2.29\n",
      "Iter-38, training loss2.28\n",
      "Iter-39, training loss2.28\n",
      "Iter-40, training loss2.28\n",
      "Iter-41, training loss2.28\n",
      "Iter-42, training loss2.29\n",
      "Iter-43, training loss2.28\n",
      "Iter-44, training loss2.28\n",
      "Iter-45, training loss2.28\n",
      "Iter-46, training loss2.28\n",
      "Iter-47, training loss2.28\n",
      "Iter-48, training loss2.26\n",
      "Iter-49, training loss2.26\n",
      "Iter-50, training loss2.27\n",
      "Iter-51, training loss2.27\n",
      "Iter-52, training loss2.27\n",
      "Iter-53, training loss2.26\n",
      "Iter-54, training loss2.25\n",
      "Iter-55, training loss2.26\n",
      "Iter-56, training loss2.25\n",
      "Iter-57, training loss2.25\n",
      "Iter-58, training loss2.25\n",
      "Iter-59, training loss2.25\n",
      "Iter-60, training loss2.25\n",
      "Iter-61, training loss2.25\n",
      "Iter-62, training loss2.23\n",
      "Iter-63, training loss2.23\n",
      "Iter-64, training loss2.24\n",
      "Iter-65, training loss2.24\n",
      "Iter-66, training loss2.23\n",
      "Iter-67, training loss2.24\n",
      "Iter-68, training loss2.21\n",
      "Iter-69, training loss2.20\n",
      "Iter-70, training loss2.22\n",
      "Iter-71, training loss2.21\n",
      "Iter-72, training loss2.22\n",
      "Iter-73, training loss2.20\n",
      "Iter-74, training loss2.20\n",
      "Iter-75, training loss2.24\n",
      "Iter-76, training loss2.22\n",
      "Iter-77, training loss2.22\n",
      "Iter-78, training loss2.20\n",
      "Iter-79, training loss2.18\n",
      "Iter-80, training loss2.17\n",
      "Iter-81, training loss2.24\n",
      "Iter-82, training loss2.18\n",
      "Iter-83, training loss2.20\n",
      "Iter-84, training loss2.17\n",
      "Iter-85, training loss2.18\n",
      "Iter-86, training loss2.22\n",
      "Iter-87, training loss2.21\n",
      "Iter-88, training loss2.16\n",
      "Iter-89, training loss2.17\n",
      "Iter-90, training loss2.14\n",
      "Iter-91, training loss2.17\n",
      "Iter-92, training loss2.15\n",
      "Iter-93, training loss2.16\n",
      "Iter-94, training loss2.16\n",
      "Iter-95, training loss2.17\n",
      "Iter-96, training loss2.11\n",
      "Iter-97, training loss2.16\n",
      "Iter-98, training loss2.18\n",
      "Iter-99, training loss2.17\n",
      "Iter-100, training loss2.15\n",
      "Iter-101, training loss2.12\n",
      "Iter-102, training loss2.13\n",
      "Iter-103, training loss2.11\n",
      "Iter-104, training loss2.14\n",
      "Iter-105, training loss2.18\n",
      "Iter-106, training loss2.15\n",
      "Iter-107, training loss2.16\n",
      "Iter-108, training loss2.17\n",
      "Iter-109, training loss2.13\n",
      "Iter-110, training loss2.15\n",
      "Iter-111, training loss2.12\n",
      "Iter-112, training loss2.12\n",
      "Iter-113, training loss2.15\n",
      "Iter-114, training loss2.13\n",
      "Iter-115, training loss2.13\n",
      "Iter-116, training loss2.12\n",
      "Iter-117, training loss2.14\n",
      "Iter-118, training loss2.12\n",
      "Iter-119, training loss2.20\n",
      "Iter-120, training loss2.09\n",
      "Iter-121, training loss2.08\n",
      "Iter-122, training loss2.09\n",
      "Iter-123, training loss2.13\n",
      "Iter-124, training loss2.10\n",
      "Iter-125, training loss2.16\n",
      "Iter-126, training loss2.10\n",
      "Iter-127, training loss2.10\n",
      "Iter-128, training loss2.12\n",
      "Iter-129, training loss2.11\n",
      "Iter-130, training loss2.09\n",
      "Iter-131, training loss2.11\n",
      "Iter-132, training loss2.07\n",
      "Iter-133, training loss2.08\n",
      "Iter-134, training loss2.15\n",
      "Iter-135, training loss2.12\n",
      "Iter-136, training loss2.11\n",
      "Iter-137, training loss2.12\n",
      "Iter-138, training loss2.13\n",
      "Iter-139, training loss2.09\n",
      "Iter-140, training loss2.00\n",
      "Iter-141, training loss2.04\n",
      "Iter-142, training loss2.04\n",
      "Iter-143, training loss2.07\n",
      "Iter-144, training loss2.06\n",
      "Iter-145, training loss2.06\n",
      "Iter-146, training loss2.12\n",
      "Iter-147, training loss2.13\n",
      "Iter-148, training loss1.97\n",
      "Iter-149, training loss2.07\n",
      "Iter-150, training loss2.08\n",
      "Iter-151, training loss2.17\n",
      "Iter-152, training loss2.06\n",
      "Iter-153, training loss2.11\n",
      "Iter-154, training loss2.05\n",
      "Iter-155, training loss2.05\n",
      "Iter-156, training loss2.12\n",
      "Iter-157, training loss2.08\n",
      "Iter-158, training loss2.09\n",
      "Iter-159, training loss2.10\n",
      "Iter-160, training loss2.05\n",
      "Iter-161, training loss2.08\n",
      "Iter-162, training loss2.07\n",
      "Iter-163, training loss2.01\n",
      "Iter-164, training loss2.06\n",
      "Iter-165, training loss2.07\n",
      "Iter-166, training loss2.03\n",
      "Iter-167, training loss2.09\n",
      "Iter-168, training loss2.10\n",
      "Iter-169, training loss2.07\n",
      "Iter-170, training loss2.06\n",
      "Iter-171, training loss2.13\n",
      "Iter-172, training loss2.05\n",
      "Iter-173, training loss2.09\n",
      "Iter-174, training loss2.01\n",
      "Iter-175, training loss2.08\n",
      "Iter-176, training loss2.03\n",
      "Iter-177, training loss2.03\n",
      "Iter-178, training loss2.01\n",
      "Iter-179, training loss2.04\n",
      "Iter-180, training loss2.05\n",
      "Iter-181, training loss2.02\n",
      "Iter-182, training loss2.00\n",
      "Iter-183, training loss2.06\n",
      "Iter-184, training loss2.06\n",
      "Iter-185, training loss2.24\n",
      "Iter-186, training loss2.07\n",
      "Iter-187, training loss2.04\n",
      "Iter-188, training loss2.11\n",
      "Iter-189, training loss2.07\n",
      "Iter-190, training loss2.06\n",
      "Iter-191, training loss2.03\n",
      "Iter-192, training loss1.93\n",
      "Iter-193, training loss2.11\n",
      "Iter-194, training loss2.05\n",
      "Iter-195, training loss2.09\n",
      "Iter-196, training loss2.14\n",
      "Iter-197, training loss2.03\n",
      "Iter-198, training loss2.03\n",
      "Iter-199, training loss2.04\n",
      "Iter-200, training loss1.94\n",
      "Iter-201, training loss2.03\n",
      "Iter-202, training loss2.01\n",
      "Iter-203, training loss2.10\n",
      "Iter-204, training loss1.96\n",
      "Iter-205, training loss2.03\n",
      "Iter-206, training loss2.02\n",
      "Iter-207, training loss2.03\n",
      "Iter-208, training loss2.12\n",
      "Iter-209, training loss2.00\n",
      "Iter-210, training loss2.00\n",
      "Iter-211, training loss1.94\n",
      "Iter-212, training loss2.01\n",
      "Iter-213, training loss1.99\n",
      "Iter-214, training loss2.01\n",
      "Iter-215, training loss1.99\n",
      "Iter-216, training loss1.97\n",
      "Iter-217, training loss2.07\n",
      "Iter-218, training loss2.07\n",
      "Iter-219, training loss1.99\n",
      "Iter-220, training loss2.03\n",
      "Iter-221, training loss2.01\n",
      "Iter-222, training loss2.05\n",
      "Iter-223, training loss1.93\n",
      "Iter-224, training loss1.98\n",
      "Iter-225, training loss2.01\n",
      "Iter-226, training loss2.06\n",
      "Iter-227, training loss2.10\n",
      "Iter-228, training loss1.97\n",
      "Iter-229, training loss2.02\n",
      "Iter-230, training loss1.95\n",
      "Iter-231, training loss2.02\n",
      "Iter-232, training loss2.11\n",
      "Iter-233, training loss2.06\n",
      "Iter-234, training loss1.98\n",
      "Iter-235, training loss1.96\n",
      "Iter-236, training loss2.03\n",
      "Iter-237, training loss2.02\n",
      "Iter-238, training loss2.02\n",
      "Iter-239, training loss2.00\n",
      "Iter-240, training loss2.07\n",
      "Iter-241, training loss1.93\n",
      "Iter-242, training loss2.04\n",
      "Iter-243, training loss1.97\n",
      "Iter-244, training loss2.00\n",
      "Iter-245, training loss2.05\n",
      "Iter-246, training loss1.91\n",
      "Iter-247, training loss1.95\n",
      "Iter-248, training loss2.01\n",
      "Iter-249, training loss2.04\n",
      "Iter-250, training loss1.95\n",
      "Iter-251, training loss2.08\n",
      "Iter-252, training loss2.06\n",
      "Iter-253, training loss1.93\n",
      "Iter-254, training loss1.96\n",
      "Iter-255, training loss2.00\n",
      "Iter-256, training loss2.05\n",
      "Iter-257, training loss1.96\n",
      "Iter-258, training loss2.02\n",
      "Iter-259, training loss2.01\n",
      "Iter-260, training loss1.93\n",
      "Iter-261, training loss1.94\n",
      "Iter-262, training loss1.99\n",
      "Iter-263, training loss2.07\n",
      "Iter-264, training loss2.00\n",
      "Iter-265, training loss2.01\n",
      "Iter-266, training loss2.11\n",
      "Iter-267, training loss1.91\n",
      "Iter-268, training loss2.10\n",
      "Iter-269, training loss2.01\n",
      "Iter-270, training loss2.01\n",
      "Iter-271, training loss2.07\n",
      "Iter-272, training loss1.97\n",
      "Iter-273, training loss2.02\n",
      "Iter-274, training loss2.04\n",
      "Iter-275, training loss2.03\n",
      "Iter-276, training loss2.03\n",
      "Iter-277, training loss1.94\n",
      "Iter-278, training loss2.01\n",
      "Iter-279, training loss1.95\n",
      "Iter-280, training loss1.92\n",
      "Iter-281, training loss1.97\n",
      "Iter-282, training loss2.03\n",
      "Iter-283, training loss2.02\n",
      "Iter-284, training loss2.06\n",
      "Iter-285, training loss1.99\n",
      "Iter-286, training loss1.91\n",
      "Iter-287, training loss1.92\n",
      "Iter-288, training loss2.03\n",
      "Iter-289, training loss2.10\n",
      "Iter-290, training loss1.93\n",
      "Iter-291, training loss1.97\n",
      "Iter-292, training loss1.89\n",
      "Iter-293, training loss1.96\n",
      "Iter-294, training loss1.84\n",
      "Iter-295, training loss1.91\n",
      "Iter-296, training loss2.03\n",
      "Iter-297, training loss1.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-298, training loss2.02\n",
      "Iter-299, training loss2.06\n",
      "Iter-300, training loss2.03\n",
      "Iter-301, training loss2.11\n",
      "Iter-302, training loss2.03\n",
      "Iter-303, training loss2.03\n",
      "Iter-304, training loss1.94\n",
      "Iter-305, training loss2.06\n",
      "Iter-306, training loss2.01\n",
      "Iter-307, training loss1.91\n",
      "Iter-308, training loss2.00\n",
      "Iter-309, training loss1.95\n",
      "Iter-310, training loss1.95\n",
      "Iter-311, training loss1.94\n",
      "Iter-312, training loss1.91\n",
      "Iter-313, training loss2.01\n",
      "Iter-314, training loss1.89\n",
      "Iter-315, training loss2.03\n",
      "Iter-316, training loss1.93\n",
      "Iter-317, training loss1.89\n",
      "Iter-318, training loss2.00\n",
      "Iter-319, training loss2.04\n",
      "Iter-320, training loss2.03\n",
      "Iter-321, training loss1.96\n",
      "Iter-322, training loss2.04\n",
      "Iter-323, training loss1.94\n",
      "Iter-324, training loss2.01\n",
      "Iter-325, training loss1.98\n",
      "Iter-326, training loss1.87\n",
      "Iter-327, training loss1.95\n",
      "Iter-328, training loss2.08\n",
      "Iter-329, training loss1.98\n",
      "Iter-330, training loss1.90\n",
      "Iter-331, training loss1.97\n",
      "Iter-332, training loss1.90\n",
      "Iter-333, training loss1.92\n",
      "Iter-334, training loss1.87\n",
      "Iter-335, training loss1.98\n",
      "Iter-336, training loss1.92\n",
      "Iter-337, training loss1.98\n",
      "Iter-338, training loss2.10\n",
      "Iter-339, training loss1.99\n",
      "Iter-340, training loss1.92\n",
      "Iter-341, training loss1.95\n",
      "Iter-342, training loss2.05\n",
      "Iter-343, training loss2.00\n",
      "Iter-344, training loss2.04\n",
      "Iter-345, training loss1.98\n",
      "Iter-346, training loss1.96\n",
      "Iter-347, training loss1.94\n",
      "Iter-348, training loss2.06\n",
      "Iter-349, training loss1.95\n",
      "Iter-350, training loss1.98\n",
      "Iter-351, training loss1.81\n",
      "Iter-352, training loss1.91\n",
      "Iter-353, training loss1.98\n",
      "Iter-354, training loss2.01\n",
      "Iter-355, training loss2.02\n",
      "Iter-356, training loss1.95\n",
      "Iter-357, training loss2.09\n",
      "Iter-358, training loss1.94\n",
      "Iter-359, training loss1.95\n",
      "Iter-360, training loss2.12\n",
      "Iter-361, training loss2.05\n",
      "Iter-362, training loss1.98\n",
      "Iter-363, training loss1.90\n",
      "Iter-364, training loss1.95\n",
      "Iter-365, training loss1.88\n",
      "Iter-366, training loss1.94\n",
      "Iter-367, training loss2.09\n",
      "Iter-368, training loss2.03\n",
      "Iter-369, training loss1.86\n",
      "Iter-370, training loss2.01\n",
      "Iter-371, training loss1.96\n",
      "Iter-372, training loss1.96\n",
      "Iter-373, training loss1.97\n",
      "Iter-374, training loss1.97\n",
      "Iter-375, training loss1.97\n",
      "Iter-376, training loss1.95\n",
      "Iter-377, training loss1.94\n",
      "Iter-378, training loss1.86\n",
      "Iter-379, training loss1.88\n",
      "Iter-380, training loss1.94\n",
      "Iter-381, training loss1.89\n",
      "Iter-382, training loss1.89\n",
      "Iter-383, training loss1.93\n",
      "Iter-384, training loss1.94\n",
      "Iter-385, training loss1.91\n",
      "Iter-386, training loss1.99\n",
      "Iter-387, training loss1.89\n",
      "Iter-388, training loss2.02\n",
      "Iter-389, training loss2.04\n",
      "Iter-390, training loss1.99\n",
      "Iter-391, training loss2.00\n",
      "Iter-392, training loss2.03\n",
      "Iter-393, training loss2.06\n",
      "Iter-394, training loss2.11\n",
      "Iter-395, training loss1.89\n",
      "Iter-396, training loss1.90\n",
      "Iter-397, training loss1.98\n",
      "Iter-398, training loss2.00\n",
      "Iter-399, training loss1.83\n",
      "Iter-400, training loss1.94\n",
      "Iter-401, training loss2.06\n",
      "Iter-402, training loss1.95\n",
      "Iter-403, training loss1.98\n",
      "Iter-404, training loss2.03\n",
      "Iter-405, training loss1.95\n",
      "Iter-406, training loss1.99\n",
      "Iter-407, training loss2.00\n",
      "Iter-408, training loss1.96\n",
      "Iter-409, training loss1.91\n",
      "Iter-410, training loss1.98\n",
      "Iter-411, training loss1.90\n",
      "Iter-412, training loss1.93\n",
      "Iter-413, training loss1.88\n",
      "Iter-414, training loss1.99\n",
      "Iter-415, training loss1.88\n",
      "Iter-416, training loss1.90\n",
      "Iter-417, training loss1.95\n",
      "Iter-418, training loss1.91\n",
      "Iter-419, training loss1.99\n",
      "Iter-420, training loss1.95\n",
      "Iter-421, training loss1.91\n",
      "Iter-422, training loss1.92\n",
      "Iter-423, training loss1.98\n",
      "Iter-424, training loss1.94\n",
      "Iter-425, training loss2.02\n",
      "Iter-426, training loss1.93\n",
      "Iter-427, training loss1.94\n",
      "Iter-428, training loss1.99\n",
      "Iter-429, training loss1.91\n",
      "Iter-430, training loss2.06\n",
      "Iter-431, training loss1.98\n",
      "Iter-432, training loss1.91\n",
      "Iter-433, training loss1.92\n",
      "Iter-434, training loss1.95\n",
      "Iter-435, training loss2.01\n",
      "Iter-436, training loss1.91\n",
      "Iter-437, training loss1.86\n",
      "Iter-438, training loss1.88\n",
      "Iter-439, training loss1.97\n",
      "Iter-440, training loss1.87\n",
      "Iter-441, training loss1.94\n",
      "Iter-442, training loss1.97\n",
      "Iter-443, training loss1.93\n",
      "Iter-444, training loss1.86\n",
      "Iter-445, training loss2.05\n",
      "Iter-446, training loss1.98\n",
      "Iter-447, training loss1.98\n",
      "Iter-448, training loss1.91\n",
      "Iter-449, training loss1.95\n",
      "Iter-450, training loss1.90\n",
      "Iter-451, training loss1.86\n",
      "Iter-452, training loss1.82\n",
      "Iter-453, training loss1.82\n",
      "Iter-454, training loss1.92\n",
      "Iter-455, training loss1.93\n",
      "Iter-456, training loss2.03\n",
      "Iter-457, training loss1.93\n",
      "Iter-458, training loss1.91\n",
      "Iter-459, training loss1.83\n",
      "Iter-460, training loss1.93\n",
      "Iter-461, training loss1.89\n",
      "Iter-462, training loss1.98\n",
      "Iter-463, training loss1.93\n",
      "Iter-464, training loss2.16\n",
      "Iter-465, training loss1.92\n",
      "Iter-466, training loss1.98\n",
      "Iter-467, training loss1.90\n",
      "Iter-468, training loss1.80\n",
      "Iter-469, training loss1.90\n",
      "Iter-470, training loss2.03\n",
      "Iter-471, training loss1.95\n",
      "Iter-472, training loss1.98\n",
      "Iter-473, training loss1.92\n",
      "Iter-474, training loss1.87\n",
      "Iter-475, training loss2.04\n",
      "Iter-476, training loss1.88\n",
      "Iter-477, training loss1.91\n",
      "Iter-478, training loss1.94\n",
      "Iter-479, training loss1.88\n",
      "Iter-480, training loss2.04\n",
      "Iter-481, training loss1.94\n",
      "Iter-482, training loss1.92\n",
      "Iter-483, training loss1.92\n",
      "Iter-484, training loss1.98\n",
      "Iter-485, training loss1.95\n",
      "Iter-486, training loss1.85\n",
      "Iter-487, training loss1.95\n",
      "Iter-488, training loss1.94\n",
      "Iter-489, training loss2.07\n",
      "Iter-490, training loss1.95\n",
      "Iter-491, training loss1.92\n",
      "Iter-492, training loss2.03\n",
      "Iter-493, training loss1.87\n",
      "Iter-494, training loss1.83\n",
      "Iter-495, training loss1.98\n",
      "Iter-496, training loss1.96\n",
      "Iter-497, training loss1.92\n",
      "Iter-498, training loss2.00\n",
      "Iter-499, training loss1.85\n",
      "Iter-500, training loss1.95\n",
      "Epoch-2\n",
      "-----------------\n",
      "Iter-501, training loss2.02\n",
      "Iter-502, training loss1.87\n",
      "Iter-503, training loss1.86\n",
      "Iter-504, training loss2.00\n",
      "Iter-505, training loss2.04\n",
      "Iter-506, training loss1.87\n",
      "Iter-507, training loss1.83\n",
      "Iter-508, training loss1.88\n",
      "Iter-509, training loss1.94\n",
      "Iter-510, training loss1.80\n",
      "Iter-511, training loss1.98\n",
      "Iter-512, training loss1.77\n",
      "Iter-513, training loss1.88\n",
      "Iter-514, training loss1.97\n",
      "Iter-515, training loss1.95\n",
      "Iter-516, training loss1.99\n",
      "Iter-517, training loss1.85\n",
      "Iter-518, training loss1.92\n",
      "Iter-519, training loss2.02\n",
      "Iter-520, training loss1.80\n",
      "Iter-521, training loss1.86\n",
      "Iter-522, training loss1.96\n",
      "Iter-523, training loss1.90\n",
      "Iter-524, training loss2.00\n",
      "Iter-525, training loss1.91\n",
      "Iter-526, training loss1.89\n",
      "Iter-527, training loss1.95\n",
      "Iter-528, training loss2.02\n",
      "Iter-529, training loss1.89\n",
      "Iter-530, training loss1.85\n",
      "Iter-531, training loss1.92\n",
      "Iter-532, training loss1.80\n",
      "Iter-533, training loss1.94\n",
      "Iter-534, training loss1.85\n",
      "Iter-535, training loss1.83\n",
      "Iter-536, training loss2.01\n",
      "Iter-537, training loss1.89\n",
      "Iter-538, training loss1.96\n",
      "Iter-539, training loss2.00\n",
      "Iter-540, training loss1.92\n",
      "Iter-541, training loss1.92\n",
      "Iter-542, training loss1.88\n",
      "Iter-543, training loss1.94\n",
      "Iter-544, training loss1.98\n",
      "Iter-545, training loss1.94\n",
      "Iter-546, training loss1.96\n",
      "Iter-547, training loss1.93\n",
      "Iter-548, training loss1.87\n",
      "Iter-549, training loss2.01\n",
      "Iter-550, training loss1.96\n",
      "Iter-551, training loss1.93\n",
      "Iter-552, training loss1.85\n",
      "Iter-553, training loss1.96\n",
      "Iter-554, training loss2.12\n",
      "Iter-555, training loss1.96\n",
      "Iter-556, training loss1.94\n",
      "Iter-557, training loss1.95\n",
      "Iter-558, training loss1.85\n",
      "Iter-559, training loss1.95\n",
      "Iter-560, training loss1.98\n",
      "Iter-561, training loss1.89\n",
      "Iter-562, training loss1.82\n",
      "Iter-563, training loss1.93\n",
      "Iter-564, training loss2.00\n",
      "Iter-565, training loss2.00\n",
      "Iter-566, training loss2.03\n",
      "Iter-567, training loss1.84\n",
      "Iter-568, training loss1.86\n",
      "Iter-569, training loss1.87\n",
      "Iter-570, training loss2.07\n",
      "Iter-571, training loss1.97\n",
      "Iter-572, training loss2.04\n",
      "Iter-573, training loss1.85\n",
      "Iter-574, training loss1.75\n",
      "Iter-575, training loss1.94\n",
      "Iter-576, training loss1.90\n",
      "Iter-577, training loss1.82\n",
      "Iter-578, training loss1.99\n",
      "Iter-579, training loss1.86\n",
      "Iter-580, training loss2.03\n",
      "Iter-581, training loss1.85\n",
      "Iter-582, training loss1.91\n",
      "Iter-583, training loss1.90\n",
      "Iter-584, training loss2.00\n",
      "Iter-585, training loss2.00\n",
      "Iter-586, training loss1.99\n",
      "Iter-587, training loss1.90\n",
      "Iter-588, training loss1.95\n",
      "Iter-589, training loss1.89\n",
      "Iter-590, training loss1.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-591, training loss1.81\n",
      "Iter-592, training loss1.98\n",
      "Iter-593, training loss1.88\n",
      "Iter-594, training loss1.84\n",
      "Iter-595, training loss2.01\n",
      "Iter-596, training loss1.89\n",
      "Iter-597, training loss1.98\n",
      "Iter-598, training loss1.95\n",
      "Iter-599, training loss2.07\n",
      "Iter-600, training loss1.84\n",
      "Iter-601, training loss1.96\n",
      "Iter-602, training loss1.93\n",
      "Iter-603, training loss1.92\n",
      "Iter-604, training loss2.01\n",
      "Iter-605, training loss1.87\n",
      "Iter-606, training loss1.86\n",
      "Iter-607, training loss1.92\n",
      "Iter-608, training loss1.91\n",
      "Iter-609, training loss1.95\n",
      "Iter-610, training loss1.93\n",
      "Iter-611, training loss1.94\n",
      "Iter-612, training loss1.88\n",
      "Iter-613, training loss1.91\n",
      "Iter-614, training loss1.92\n",
      "Iter-615, training loss1.96\n",
      "Iter-616, training loss1.82\n",
      "Iter-617, training loss1.97\n",
      "Iter-618, training loss1.88\n",
      "Iter-619, training loss2.01\n",
      "Iter-620, training loss1.85\n",
      "Iter-621, training loss1.82\n",
      "Iter-622, training loss1.93\n",
      "Iter-623, training loss1.93\n",
      "Iter-624, training loss1.93\n",
      "Iter-625, training loss1.92\n",
      "Iter-626, training loss1.92\n",
      "Iter-627, training loss1.97\n",
      "Iter-628, training loss1.94\n",
      "Iter-629, training loss1.87\n",
      "Iter-630, training loss1.95\n",
      "Iter-631, training loss1.96\n",
      "Iter-632, training loss1.93\n",
      "Iter-633, training loss1.88\n",
      "Iter-634, training loss1.96\n",
      "Iter-635, training loss2.00\n",
      "Iter-636, training loss1.86\n",
      "Iter-637, training loss1.89\n",
      "Iter-638, training loss1.83\n",
      "Iter-639, training loss2.03\n",
      "Iter-640, training loss1.84\n",
      "Iter-641, training loss1.99\n",
      "Iter-642, training loss1.94\n",
      "Iter-643, training loss1.92\n",
      "Iter-644, training loss1.90\n",
      "Iter-645, training loss1.85\n",
      "Iter-646, training loss1.85\n",
      "Iter-647, training loss1.98\n",
      "Iter-648, training loss1.91\n",
      "Iter-649, training loss2.08\n",
      "Iter-650, training loss1.93\n",
      "Iter-651, training loss1.97\n",
      "Iter-652, training loss1.76\n",
      "Iter-653, training loss1.90\n",
      "Iter-654, training loss1.93\n",
      "Iter-655, training loss1.87\n",
      "Iter-656, training loss1.85\n",
      "Iter-657, training loss1.84\n",
      "Iter-658, training loss1.96\n",
      "Iter-659, training loss1.97\n",
      "Iter-660, training loss1.99\n",
      "Iter-661, training loss1.94\n",
      "Iter-662, training loss1.90\n",
      "Iter-663, training loss1.95\n",
      "Iter-664, training loss1.94\n",
      "Iter-665, training loss1.95\n",
      "Iter-666, training loss1.83\n",
      "Iter-667, training loss1.78\n",
      "Iter-668, training loss1.81\n",
      "Iter-669, training loss1.89\n",
      "Iter-670, training loss1.84\n",
      "Iter-671, training loss1.91\n",
      "Iter-672, training loss1.95\n",
      "Iter-673, training loss1.86\n",
      "Iter-674, training loss1.84\n",
      "Iter-675, training loss2.02\n",
      "Iter-676, training loss2.03\n",
      "Iter-677, training loss1.88\n",
      "Iter-678, training loss1.91\n",
      "Iter-679, training loss1.79\n",
      "Iter-680, training loss1.94\n",
      "Iter-681, training loss1.91\n",
      "Iter-682, training loss1.89\n",
      "Iter-683, training loss1.85\n",
      "Iter-684, training loss2.10\n",
      "Iter-685, training loss1.93\n",
      "Iter-686, training loss1.96\n",
      "Iter-687, training loss1.90\n",
      "Iter-688, training loss1.92\n",
      "Iter-689, training loss1.89\n",
      "Iter-690, training loss1.90\n",
      "Iter-691, training loss1.77\n",
      "Iter-692, training loss1.92\n",
      "Iter-693, training loss1.92\n",
      "Iter-694, training loss1.91\n",
      "Iter-695, training loss1.90\n",
      "Iter-696, training loss2.01\n",
      "Iter-697, training loss1.99\n",
      "Iter-698, training loss1.95\n",
      "Iter-699, training loss1.89\n",
      "Iter-700, training loss1.88\n",
      "Iter-701, training loss1.91\n",
      "Iter-702, training loss1.87\n",
      "Iter-703, training loss1.89\n",
      "Iter-704, training loss1.92\n",
      "Iter-705, training loss1.93\n",
      "Iter-706, training loss1.96\n",
      "Iter-707, training loss1.91\n",
      "Iter-708, training loss1.96\n",
      "Iter-709, training loss1.99\n",
      "Iter-710, training loss1.95\n",
      "Iter-711, training loss1.79\n",
      "Iter-712, training loss1.78\n",
      "Iter-713, training loss1.90\n",
      "Iter-714, training loss1.84\n",
      "Iter-715, training loss1.81\n",
      "Iter-716, training loss1.91\n",
      "Iter-717, training loss1.94\n",
      "Iter-718, training loss1.84\n",
      "Iter-719, training loss1.89\n",
      "Iter-720, training loss1.89\n",
      "Iter-721, training loss1.88\n",
      "Iter-722, training loss1.91\n",
      "Iter-723, training loss1.80\n",
      "Iter-724, training loss2.08\n",
      "Iter-725, training loss1.88\n",
      "Iter-726, training loss1.89\n",
      "Iter-727, training loss1.86\n",
      "Iter-728, training loss1.86\n",
      "Iter-729, training loss1.96\n",
      "Iter-730, training loss1.65\n",
      "Iter-731, training loss1.92\n",
      "Iter-732, training loss1.81\n",
      "Iter-733, training loss2.00\n",
      "Iter-734, training loss1.92\n",
      "Iter-735, training loss1.94\n",
      "Iter-736, training loss1.75\n",
      "Iter-737, training loss2.01\n",
      "Iter-738, training loss2.02\n",
      "Iter-739, training loss1.97\n",
      "Iter-740, training loss1.85\n",
      "Iter-741, training loss1.99\n",
      "Iter-742, training loss2.02\n",
      "Iter-743, training loss1.92\n",
      "Iter-744, training loss1.98\n",
      "Iter-745, training loss1.82\n",
      "Iter-746, training loss1.81\n",
      "Iter-747, training loss1.96\n",
      "Iter-748, training loss1.88\n",
      "Iter-749, training loss1.92\n",
      "Iter-750, training loss1.99\n",
      "Iter-751, training loss2.02\n",
      "Iter-752, training loss2.00\n",
      "Iter-753, training loss1.91\n",
      "Iter-754, training loss1.93\n",
      "Iter-755, training loss1.88\n",
      "Iter-756, training loss1.85\n",
      "Iter-757, training loss1.84\n",
      "Iter-758, training loss1.94\n",
      "Iter-759, training loss2.11\n",
      "Iter-760, training loss1.90\n",
      "Iter-761, training loss1.92\n",
      "Iter-762, training loss1.91\n",
      "Iter-763, training loss1.91\n",
      "Iter-764, training loss1.97\n",
      "Iter-765, training loss1.86\n",
      "Iter-766, training loss1.80\n",
      "Iter-767, training loss1.79\n",
      "Iter-768, training loss1.71\n",
      "Iter-769, training loss1.82\n",
      "Iter-770, training loss1.93\n",
      "Iter-771, training loss1.85\n",
      "Iter-772, training loss1.76\n",
      "Iter-773, training loss1.97\n",
      "Iter-774, training loss2.02\n",
      "Iter-775, training loss2.05\n",
      "Iter-776, training loss2.07\n",
      "Iter-777, training loss1.92\n",
      "Iter-778, training loss1.85\n",
      "Iter-779, training loss1.86\n",
      "Iter-780, training loss1.83\n",
      "Iter-781, training loss1.80\n",
      "Iter-782, training loss1.80\n",
      "Iter-783, training loss2.10\n",
      "Iter-784, training loss1.88\n",
      "Iter-785, training loss1.83\n",
      "Iter-786, training loss1.94\n",
      "Iter-787, training loss1.78\n",
      "Iter-788, training loss1.83\n",
      "Iter-789, training loss1.94\n",
      "Iter-790, training loss1.81\n",
      "Iter-791, training loss1.83\n",
      "Iter-792, training loss1.99\n",
      "Iter-793, training loss1.92\n",
      "Iter-794, training loss1.80\n",
      "Iter-795, training loss1.87\n",
      "Iter-796, training loss1.90\n",
      "Iter-797, training loss1.93\n",
      "Iter-798, training loss1.92\n",
      "Iter-799, training loss1.79\n",
      "Iter-800, training loss1.94\n",
      "Iter-801, training loss1.91\n",
      "Iter-802, training loss1.83\n",
      "Iter-803, training loss1.94\n",
      "Iter-804, training loss2.02\n",
      "Iter-805, training loss1.84\n",
      "Iter-806, training loss1.85\n",
      "Iter-807, training loss2.03\n",
      "Iter-808, training loss2.08\n",
      "Iter-809, training loss1.87\n",
      "Iter-810, training loss1.98\n",
      "Iter-811, training loss1.87\n",
      "Iter-812, training loss2.07\n",
      "Iter-813, training loss1.79\n",
      "Iter-814, training loss2.09\n",
      "Iter-815, training loss1.86\n",
      "Iter-816, training loss1.93\n",
      "Iter-817, training loss1.76\n",
      "Iter-818, training loss1.81\n",
      "Iter-819, training loss1.95\n",
      "Iter-820, training loss1.95\n",
      "Iter-821, training loss1.86\n",
      "Iter-822, training loss1.97\n",
      "Iter-823, training loss1.80\n",
      "Iter-824, training loss1.95\n",
      "Iter-825, training loss1.86\n",
      "Iter-826, training loss1.92\n",
      "Iter-827, training loss1.78\n",
      "Iter-828, training loss1.86\n",
      "Iter-829, training loss1.86\n",
      "Iter-830, training loss2.08\n",
      "Iter-831, training loss1.82\n",
      "Iter-832, training loss1.86\n",
      "Iter-833, training loss1.81\n",
      "Iter-834, training loss1.93\n",
      "Iter-835, training loss1.88\n",
      "Iter-836, training loss1.90\n",
      "Iter-837, training loss1.61\n",
      "Iter-838, training loss1.85\n",
      "Iter-839, training loss1.82\n",
      "Iter-840, training loss1.97\n",
      "Iter-841, training loss1.95\n",
      "Iter-842, training loss1.81\n",
      "Iter-843, training loss1.97\n",
      "Iter-844, training loss1.70\n",
      "Iter-845, training loss1.80\n",
      "Iter-846, training loss2.07\n",
      "Iter-847, training loss2.07\n",
      "Iter-848, training loss1.91\n",
      "Iter-849, training loss1.88\n",
      "Iter-850, training loss1.91\n",
      "Iter-851, training loss1.88\n",
      "Iter-852, training loss1.95\n",
      "Iter-853, training loss1.88\n",
      "Iter-854, training loss1.92\n",
      "Iter-855, training loss2.08\n",
      "Iter-856, training loss1.87\n",
      "Iter-857, training loss1.79\n",
      "Iter-858, training loss1.93\n",
      "Iter-859, training loss1.98\n",
      "Iter-860, training loss1.90\n",
      "Iter-861, training loss2.00\n",
      "Iter-862, training loss1.83\n",
      "Iter-863, training loss1.78\n",
      "Iter-864, training loss1.83\n",
      "Iter-865, training loss1.92\n",
      "Iter-866, training loss1.89\n",
      "Iter-867, training loss1.87\n",
      "Iter-868, training loss1.74\n",
      "Iter-869, training loss1.95\n",
      "Iter-870, training loss1.91\n",
      "Iter-871, training loss1.93\n",
      "Iter-872, training loss1.84\n",
      "Iter-873, training loss1.86\n",
      "Iter-874, training loss1.91\n",
      "Iter-875, training loss1.85\n",
      "Iter-876, training loss1.97\n",
      "Iter-877, training loss1.97\n",
      "Iter-878, training loss1.91\n",
      "Iter-879, training loss1.97\n",
      "Iter-880, training loss1.77\n",
      "Iter-881, training loss1.86\n",
      "Iter-882, training loss1.96\n",
      "Iter-883, training loss1.94\n",
      "Iter-884, training loss1.94\n",
      "Iter-885, training loss1.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-886, training loss1.87\n",
      "Iter-887, training loss2.03\n",
      "Iter-888, training loss1.84\n",
      "Iter-889, training loss1.85\n",
      "Iter-890, training loss1.84\n",
      "Iter-891, training loss1.75\n",
      "Iter-892, training loss1.76\n",
      "Iter-893, training loss1.81\n",
      "Iter-894, training loss1.81\n",
      "Iter-895, training loss1.87\n",
      "Iter-896, training loss1.92\n",
      "Iter-897, training loss1.78\n",
      "Iter-898, training loss1.80\n",
      "Iter-899, training loss1.89\n",
      "Iter-900, training loss1.86\n",
      "Iter-901, training loss1.82\n",
      "Iter-902, training loss1.96\n",
      "Iter-903, training loss1.76\n",
      "Iter-904, training loss1.86\n",
      "Iter-905, training loss1.86\n",
      "Iter-906, training loss1.87\n",
      "Iter-907, training loss1.79\n",
      "Iter-908, training loss1.75\n",
      "Iter-909, training loss2.07\n",
      "Iter-910, training loss1.88\n",
      "Iter-911, training loss1.88\n",
      "Iter-912, training loss1.96\n",
      "Iter-913, training loss1.81\n",
      "Iter-914, training loss1.85\n",
      "Iter-915, training loss1.99\n",
      "Iter-916, training loss1.84\n",
      "Iter-917, training loss1.86\n",
      "Iter-918, training loss1.83\n",
      "Iter-919, training loss1.89\n",
      "Iter-920, training loss1.97\n",
      "Iter-921, training loss1.75\n",
      "Iter-922, training loss2.04\n",
      "Iter-923, training loss1.88\n",
      "Iter-924, training loss1.86\n",
      "Iter-925, training loss1.76\n",
      "Iter-926, training loss1.75\n",
      "Iter-927, training loss1.67\n",
      "Iter-928, training loss1.90\n",
      "Iter-929, training loss1.86\n",
      "Iter-930, training loss1.88\n",
      "Iter-931, training loss1.84\n",
      "Iter-932, training loss1.76\n",
      "Iter-933, training loss1.84\n",
      "Iter-934, training loss1.85\n",
      "Iter-935, training loss1.85\n",
      "Iter-936, training loss1.88\n",
      "Iter-937, training loss1.95\n",
      "Iter-938, training loss1.78\n",
      "Iter-939, training loss1.79\n",
      "Iter-940, training loss1.88\n",
      "Iter-941, training loss1.86\n",
      "Iter-942, training loss1.86\n",
      "Iter-943, training loss1.82\n",
      "Iter-944, training loss1.76\n",
      "Iter-945, training loss1.74\n",
      "Iter-946, training loss1.88\n",
      "Iter-947, training loss1.77\n",
      "Iter-948, training loss1.87\n",
      "Iter-949, training loss1.96\n",
      "Iter-950, training loss1.88\n",
      "Iter-951, training loss1.90\n",
      "Iter-952, training loss1.82\n",
      "Iter-953, training loss1.85\n",
      "Iter-954, training loss1.86\n",
      "Iter-955, training loss1.83\n",
      "Iter-956, training loss1.78\n",
      "Iter-957, training loss1.83\n",
      "Iter-958, training loss1.72\n",
      "Iter-959, training loss1.94\n",
      "Iter-960, training loss1.89\n",
      "Iter-961, training loss1.93\n",
      "Iter-962, training loss1.73\n",
      "Iter-963, training loss2.00\n",
      "Iter-964, training loss1.89\n",
      "Iter-965, training loss1.97\n",
      "Iter-966, training loss1.84\n",
      "Iter-967, training loss2.01\n",
      "Iter-968, training loss2.00\n",
      "Iter-969, training loss1.89\n",
      "Iter-970, training loss1.97\n",
      "Iter-971, training loss1.85\n",
      "Iter-972, training loss1.82\n",
      "Iter-973, training loss1.78\n",
      "Iter-974, training loss1.87\n",
      "Iter-975, training loss1.94\n",
      "Iter-976, training loss1.85\n",
      "Iter-977, training loss1.87\n",
      "Iter-978, training loss1.89\n",
      "Iter-979, training loss1.93\n",
      "Iter-980, training loss1.89\n",
      "Iter-981, training loss1.76\n",
      "Iter-982, training loss1.98\n",
      "Iter-983, training loss1.97\n",
      "Iter-984, training loss1.73\n",
      "Iter-985, training loss1.87\n",
      "Iter-986, training loss1.86\n",
      "Iter-987, training loss1.96\n",
      "Iter-988, training loss1.80\n",
      "Iter-989, training loss1.90\n",
      "Iter-990, training loss1.81\n",
      "Iter-991, training loss1.91\n",
      "Iter-992, training loss1.78\n",
      "Iter-993, training loss1.80\n",
      "Iter-994, training loss1.90\n",
      "Iter-995, training loss1.99\n",
      "Iter-996, training loss1.83\n",
      "Iter-997, training loss1.86\n",
      "Iter-998, training loss1.73\n",
      "Iter-999, training loss1.96\n",
      "Iter-1000, training loss1.84\n",
      "Epoch-3\n",
      "-----------------\n",
      "Iter-1001, training loss1.81\n",
      "Iter-1002, training loss1.71\n",
      "Iter-1003, training loss1.71\n",
      "Iter-1004, training loss1.99\n",
      "Iter-1005, training loss1.80\n",
      "Iter-1006, training loss1.72\n",
      "Iter-1007, training loss1.88\n",
      "Iter-1008, training loss1.87\n",
      "Iter-1009, training loss1.86\n",
      "Iter-1010, training loss1.88\n",
      "Iter-1011, training loss1.70\n",
      "Iter-1012, training loss1.88\n",
      "Iter-1013, training loss1.80\n",
      "Iter-1014, training loss2.09\n",
      "Iter-1015, training loss1.85\n",
      "Iter-1016, training loss1.88\n",
      "Iter-1017, training loss2.05\n",
      "Iter-1018, training loss2.12\n",
      "Iter-1019, training loss2.04\n",
      "Iter-1020, training loss1.81\n",
      "Iter-1021, training loss1.82\n",
      "Iter-1022, training loss1.82\n",
      "Iter-1023, training loss1.99\n",
      "Iter-1024, training loss1.97\n",
      "Iter-1025, training loss1.84\n",
      "Iter-1026, training loss1.99\n",
      "Iter-1027, training loss1.87\n",
      "Iter-1028, training loss1.83\n",
      "Iter-1029, training loss1.69\n",
      "Iter-1030, training loss1.86\n",
      "Iter-1031, training loss1.85\n",
      "Iter-1032, training loss1.74\n",
      "Iter-1033, training loss1.88\n",
      "Iter-1034, training loss1.77\n",
      "Iter-1035, training loss1.87\n",
      "Iter-1036, training loss1.82\n",
      "Iter-1037, training loss1.83\n",
      "Iter-1038, training loss1.88\n",
      "Iter-1039, training loss1.80\n",
      "Iter-1040, training loss1.90\n",
      "Iter-1041, training loss1.96\n",
      "Iter-1042, training loss1.92\n",
      "Iter-1043, training loss1.98\n",
      "Iter-1044, training loss1.75\n",
      "Iter-1045, training loss1.74\n",
      "Iter-1046, training loss1.83\n",
      "Iter-1047, training loss1.94\n",
      "Iter-1048, training loss1.87\n",
      "Iter-1049, training loss1.90\n",
      "Iter-1050, training loss1.88\n",
      "Iter-1051, training loss1.86\n",
      "Iter-1052, training loss1.87\n",
      "Iter-1053, training loss1.81\n",
      "Iter-1054, training loss1.89\n",
      "Iter-1055, training loss1.89\n",
      "Iter-1056, training loss1.89\n",
      "Iter-1057, training loss1.74\n",
      "Iter-1058, training loss1.89\n",
      "Iter-1059, training loss1.87\n",
      "Iter-1060, training loss1.71\n",
      "Iter-1061, training loss1.82\n",
      "Iter-1062, training loss1.88\n",
      "Iter-1063, training loss2.03\n",
      "Iter-1064, training loss1.84\n",
      "Iter-1065, training loss1.72\n",
      "Iter-1066, training loss1.81\n",
      "Iter-1067, training loss1.91\n",
      "Iter-1068, training loss1.79\n",
      "Iter-1069, training loss1.75\n",
      "Iter-1070, training loss1.80\n",
      "Iter-1071, training loss1.67\n",
      "Iter-1072, training loss1.85\n",
      "Iter-1073, training loss1.82\n",
      "Iter-1074, training loss1.79\n",
      "Iter-1075, training loss1.95\n",
      "Iter-1076, training loss2.02\n",
      "Iter-1077, training loss1.84\n",
      "Iter-1078, training loss1.87\n",
      "Iter-1079, training loss1.87\n",
      "Iter-1080, training loss1.84\n",
      "Iter-1081, training loss1.84\n",
      "Iter-1082, training loss1.77\n",
      "Iter-1083, training loss1.79\n",
      "Iter-1084, training loss1.83\n",
      "Iter-1085, training loss1.82\n",
      "Iter-1086, training loss1.83\n",
      "Iter-1087, training loss2.02\n",
      "Iter-1088, training loss1.86\n",
      "Iter-1089, training loss1.85\n",
      "Iter-1090, training loss1.71\n",
      "Iter-1091, training loss1.94\n",
      "Iter-1092, training loss1.80\n",
      "Iter-1093, training loss1.87\n",
      "Iter-1094, training loss1.78\n",
      "Iter-1095, training loss1.99\n",
      "Iter-1096, training loss1.80\n",
      "Iter-1097, training loss1.70\n",
      "Iter-1098, training loss1.95\n",
      "Iter-1099, training loss1.84\n",
      "Iter-1100, training loss1.87\n",
      "Iter-1101, training loss1.93\n",
      "Iter-1102, training loss1.80\n",
      "Iter-1103, training loss1.74\n",
      "Iter-1104, training loss2.03\n",
      "Iter-1105, training loss1.80\n",
      "Iter-1106, training loss1.98\n",
      "Iter-1107, training loss1.98\n",
      "Iter-1108, training loss1.78\n",
      "Iter-1109, training loss1.72\n",
      "Iter-1110, training loss1.97\n",
      "Iter-1111, training loss1.77\n",
      "Iter-1112, training loss1.70\n",
      "Iter-1113, training loss1.81\n",
      "Iter-1114, training loss1.78\n",
      "Iter-1115, training loss1.87\n",
      "Iter-1116, training loss1.73\n",
      "Iter-1117, training loss1.80\n",
      "Iter-1118, training loss1.74\n",
      "Iter-1119, training loss1.84\n",
      "Iter-1120, training loss1.83\n",
      "Iter-1121, training loss1.84\n",
      "Iter-1122, training loss1.73\n",
      "Iter-1123, training loss1.83\n",
      "Iter-1124, training loss1.92\n",
      "Iter-1125, training loss1.90\n",
      "Iter-1126, training loss1.92\n",
      "Iter-1127, training loss1.76\n",
      "Iter-1128, training loss1.85\n",
      "Iter-1129, training loss1.89\n",
      "Iter-1130, training loss1.80\n",
      "Iter-1131, training loss1.89\n",
      "Iter-1132, training loss1.92\n",
      "Iter-1133, training loss1.75\n",
      "Iter-1134, training loss1.82\n",
      "Iter-1135, training loss1.88\n",
      "Iter-1136, training loss1.98\n",
      "Iter-1137, training loss1.75\n",
      "Iter-1138, training loss1.85\n",
      "Iter-1139, training loss1.78\n",
      "Iter-1140, training loss1.89\n",
      "Iter-1141, training loss1.73\n",
      "Iter-1142, training loss1.76\n",
      "Iter-1143, training loss1.86\n",
      "Iter-1144, training loss1.99\n",
      "Iter-1145, training loss1.81\n",
      "Iter-1146, training loss1.90\n",
      "Iter-1147, training loss1.88\n",
      "Iter-1148, training loss1.79\n",
      "Iter-1149, training loss1.76\n",
      "Iter-1150, training loss1.83\n",
      "Iter-1151, training loss1.83\n",
      "Iter-1152, training loss1.88\n",
      "Iter-1153, training loss1.87\n",
      "Iter-1154, training loss1.72\n",
      "Iter-1155, training loss1.81\n",
      "Iter-1156, training loss1.88\n",
      "Iter-1157, training loss1.84\n",
      "Iter-1158, training loss1.82\n",
      "Iter-1159, training loss1.84\n",
      "Iter-1160, training loss1.86\n",
      "Iter-1161, training loss1.80\n",
      "Iter-1162, training loss1.81\n",
      "Iter-1163, training loss1.82\n",
      "Iter-1164, training loss1.79\n",
      "Iter-1165, training loss1.89\n",
      "Iter-1166, training loss1.97\n",
      "Iter-1167, training loss1.89\n",
      "Iter-1168, training loss1.79\n",
      "Iter-1169, training loss1.79\n",
      "Iter-1170, training loss1.80\n",
      "Iter-1171, training loss1.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1172, training loss1.86\n",
      "Iter-1173, training loss1.94\n",
      "Iter-1174, training loss1.97\n",
      "Iter-1175, training loss1.88\n",
      "Iter-1176, training loss1.89\n",
      "Iter-1177, training loss1.96\n",
      "Iter-1178, training loss1.64\n",
      "Iter-1179, training loss1.89\n",
      "Iter-1180, training loss1.82\n",
      "Iter-1181, training loss1.97\n",
      "Iter-1182, training loss1.84\n",
      "Iter-1183, training loss1.76\n",
      "Iter-1184, training loss1.88\n",
      "Iter-1185, training loss1.74\n",
      "Iter-1186, training loss1.94\n",
      "Iter-1187, training loss1.97\n",
      "Iter-1188, training loss1.83\n",
      "Iter-1189, training loss1.89\n",
      "Iter-1190, training loss1.94\n",
      "Iter-1191, training loss1.94\n",
      "Iter-1192, training loss1.84\n",
      "Iter-1193, training loss1.83\n",
      "Iter-1194, training loss1.88\n",
      "Iter-1195, training loss1.87\n",
      "Iter-1196, training loss1.91\n",
      "Iter-1197, training loss1.99\n",
      "Iter-1198, training loss1.97\n",
      "Iter-1199, training loss1.93\n",
      "Iter-1200, training loss1.86\n",
      "Iter-1201, training loss1.91\n",
      "Iter-1202, training loss1.49\n",
      "Iter-1203, training loss1.93\n",
      "Iter-1204, training loss1.82\n",
      "Iter-1205, training loss1.92\n",
      "Iter-1206, training loss1.90\n",
      "Iter-1207, training loss1.77\n",
      "Iter-1208, training loss1.84\n",
      "Iter-1209, training loss1.89\n",
      "Iter-1210, training loss1.96\n",
      "Iter-1211, training loss1.91\n",
      "Iter-1212, training loss1.85\n",
      "Iter-1213, training loss1.90\n",
      "Iter-1214, training loss1.84\n",
      "Iter-1215, training loss1.94\n",
      "Iter-1216, training loss1.77\n",
      "Iter-1217, training loss1.90\n",
      "Iter-1218, training loss1.75\n",
      "Iter-1219, training loss1.80\n",
      "Iter-1220, training loss1.88\n",
      "Iter-1221, training loss1.83\n",
      "Iter-1222, training loss1.80\n",
      "Iter-1223, training loss1.96\n",
      "Iter-1224, training loss1.91\n",
      "Iter-1225, training loss1.74\n",
      "Iter-1226, training loss1.96\n",
      "Iter-1227, training loss1.80\n",
      "Iter-1228, training loss1.95\n",
      "Iter-1229, training loss1.90\n",
      "Iter-1230, training loss1.86\n",
      "Iter-1231, training loss1.83\n",
      "Iter-1232, training loss1.83\n",
      "Iter-1233, training loss1.77\n",
      "Iter-1234, training loss1.88\n",
      "Iter-1235, training loss1.86\n",
      "Iter-1236, training loss1.92\n",
      "Iter-1237, training loss1.91\n",
      "Iter-1238, training loss1.80\n",
      "Iter-1239, training loss1.80\n",
      "Iter-1240, training loss1.82\n",
      "Iter-1241, training loss1.73\n",
      "Iter-1242, training loss1.91\n",
      "Iter-1243, training loss1.86\n",
      "Iter-1244, training loss1.75\n",
      "Iter-1245, training loss1.77\n",
      "Iter-1246, training loss1.83\n",
      "Iter-1247, training loss1.84\n",
      "Iter-1248, training loss1.63\n",
      "Iter-1249, training loss1.95\n",
      "Iter-1250, training loss1.84\n",
      "Iter-1251, training loss1.73\n",
      "Iter-1252, training loss1.68\n",
      "Iter-1253, training loss1.78\n",
      "Iter-1254, training loss1.86\n",
      "Iter-1255, training loss1.89\n",
      "Iter-1256, training loss1.80\n",
      "Iter-1257, training loss1.92\n",
      "Iter-1258, training loss1.74\n",
      "Iter-1259, training loss1.88\n",
      "Iter-1260, training loss1.83\n",
      "Iter-1261, training loss1.84\n",
      "Iter-1262, training loss1.95\n",
      "Iter-1263, training loss1.90\n",
      "Iter-1264, training loss1.88\n",
      "Iter-1265, training loss1.86\n",
      "Iter-1266, training loss1.81\n",
      "Iter-1267, training loss1.80\n",
      "Iter-1268, training loss2.04\n",
      "Iter-1269, training loss1.79\n",
      "Iter-1270, training loss1.92\n",
      "Iter-1271, training loss1.83\n",
      "Iter-1272, training loss1.88\n",
      "Iter-1273, training loss2.10\n",
      "Iter-1274, training loss1.98\n",
      "Iter-1275, training loss1.79\n",
      "Iter-1276, training loss1.88\n",
      "Iter-1277, training loss1.88\n",
      "Iter-1278, training loss1.88\n",
      "Iter-1279, training loss1.83\n",
      "Iter-1280, training loss1.79\n",
      "Iter-1281, training loss1.83\n",
      "Iter-1282, training loss1.90\n",
      "Iter-1283, training loss1.93\n",
      "Iter-1284, training loss1.79\n",
      "Iter-1285, training loss1.82\n",
      "Iter-1286, training loss1.87\n",
      "Iter-1287, training loss1.76\n",
      "Iter-1288, training loss1.73\n",
      "Iter-1289, training loss1.84\n",
      "Iter-1290, training loss1.84\n",
      "Iter-1291, training loss1.86\n",
      "Iter-1292, training loss1.74\n",
      "Iter-1293, training loss1.95\n",
      "Iter-1294, training loss1.91\n",
      "Iter-1295, training loss1.82\n",
      "Iter-1296, training loss1.88\n",
      "Iter-1297, training loss1.90\n",
      "Iter-1298, training loss1.90\n",
      "Iter-1299, training loss1.84\n",
      "Iter-1300, training loss1.84\n",
      "Iter-1301, training loss1.81\n",
      "Iter-1302, training loss1.72\n",
      "Iter-1303, training loss2.02\n",
      "Iter-1304, training loss1.87\n",
      "Iter-1305, training loss1.71\n",
      "Iter-1306, training loss1.99\n",
      "Iter-1307, training loss1.88\n",
      "Iter-1308, training loss1.73\n",
      "Iter-1309, training loss2.05\n",
      "Iter-1310, training loss1.87\n",
      "Iter-1311, training loss1.80\n",
      "Iter-1312, training loss1.93\n",
      "Iter-1313, training loss1.87\n",
      "Iter-1314, training loss1.85\n",
      "Iter-1315, training loss1.90\n",
      "Iter-1316, training loss1.76\n",
      "Iter-1317, training loss1.88\n",
      "Iter-1318, training loss1.70\n",
      "Iter-1319, training loss1.84\n",
      "Iter-1320, training loss1.84\n",
      "Iter-1321, training loss1.82\n",
      "Iter-1322, training loss1.80\n",
      "Iter-1323, training loss1.89\n",
      "Iter-1324, training loss1.92\n",
      "Iter-1325, training loss1.87\n",
      "Iter-1326, training loss1.86\n",
      "Iter-1327, training loss1.82\n",
      "Iter-1328, training loss1.86\n",
      "Iter-1329, training loss1.71\n",
      "Iter-1330, training loss1.85\n",
      "Iter-1331, training loss1.87\n",
      "Iter-1332, training loss1.91\n",
      "Iter-1333, training loss1.60\n",
      "Iter-1334, training loss1.74\n",
      "Iter-1335, training loss1.95\n",
      "Iter-1336, training loss1.87\n",
      "Iter-1337, training loss1.81\n",
      "Iter-1338, training loss1.83\n",
      "Iter-1339, training loss1.81\n",
      "Iter-1340, training loss1.68\n",
      "Iter-1341, training loss1.86\n",
      "Iter-1342, training loss1.90\n",
      "Iter-1343, training loss1.78\n",
      "Iter-1344, training loss1.89\n",
      "Iter-1345, training loss1.89\n",
      "Iter-1346, training loss1.82\n",
      "Iter-1347, training loss1.88\n",
      "Iter-1348, training loss1.85\n",
      "Iter-1349, training loss1.84\n",
      "Iter-1350, training loss1.87\n",
      "Iter-1351, training loss1.78\n",
      "Iter-1352, training loss1.93\n",
      "Iter-1353, training loss1.76\n",
      "Iter-1354, training loss1.71\n",
      "Iter-1355, training loss1.77\n",
      "Iter-1356, training loss1.88\n",
      "Iter-1357, training loss1.92\n",
      "Iter-1358, training loss1.82\n",
      "Iter-1359, training loss1.86\n",
      "Iter-1360, training loss1.81\n",
      "Iter-1361, training loss1.68\n",
      "Iter-1362, training loss1.87\n",
      "Iter-1363, training loss1.74\n",
      "Iter-1364, training loss1.95\n",
      "Iter-1365, training loss1.82\n",
      "Iter-1366, training loss1.90\n",
      "Iter-1367, training loss1.76\n",
      "Iter-1368, training loss2.04\n",
      "Iter-1369, training loss1.85\n",
      "Iter-1370, training loss1.83\n",
      "Iter-1371, training loss1.86\n",
      "Iter-1372, training loss2.03\n",
      "Iter-1373, training loss1.76\n",
      "Iter-1374, training loss1.82\n",
      "Iter-1375, training loss1.85\n",
      "Iter-1376, training loss1.75\n",
      "Iter-1377, training loss1.77\n",
      "Iter-1378, training loss1.84\n",
      "Iter-1379, training loss2.06\n",
      "Iter-1380, training loss1.75\n",
      "Iter-1381, training loss1.95\n",
      "Iter-1382, training loss1.87\n",
      "Iter-1383, training loss1.77\n",
      "Iter-1384, training loss1.86\n",
      "Iter-1385, training loss1.84\n",
      "Iter-1386, training loss2.04\n",
      "Iter-1387, training loss1.95\n",
      "Iter-1388, training loss1.85\n",
      "Iter-1389, training loss1.83\n",
      "Iter-1390, training loss1.73\n",
      "Iter-1391, training loss1.96\n",
      "Iter-1392, training loss1.92\n",
      "Iter-1393, training loss1.79\n",
      "Iter-1394, training loss1.92\n",
      "Iter-1395, training loss1.96\n",
      "Iter-1396, training loss1.86\n",
      "Iter-1397, training loss1.91\n",
      "Iter-1398, training loss1.84\n",
      "Iter-1399, training loss1.78\n",
      "Iter-1400, training loss1.87\n",
      "Iter-1401, training loss1.83\n",
      "Iter-1402, training loss1.78\n",
      "Iter-1403, training loss1.87\n",
      "Iter-1404, training loss1.92\n",
      "Iter-1405, training loss1.85\n",
      "Iter-1406, training loss1.88\n",
      "Iter-1407, training loss1.73\n",
      "Iter-1408, training loss1.75\n",
      "Iter-1409, training loss1.97\n",
      "Iter-1410, training loss1.75\n",
      "Iter-1411, training loss1.83\n",
      "Iter-1412, training loss1.88\n",
      "Iter-1413, training loss1.86\n",
      "Iter-1414, training loss1.78\n",
      "Iter-1415, training loss1.93\n",
      "Iter-1416, training loss1.93\n",
      "Iter-1417, training loss1.77\n",
      "Iter-1418, training loss1.89\n",
      "Iter-1419, training loss1.87\n",
      "Iter-1420, training loss1.78\n",
      "Iter-1421, training loss2.01\n",
      "Iter-1422, training loss1.80\n",
      "Iter-1423, training loss1.82\n",
      "Iter-1424, training loss1.79\n",
      "Iter-1425, training loss1.90\n",
      "Iter-1426, training loss1.91\n",
      "Iter-1427, training loss1.85\n",
      "Iter-1428, training loss1.90\n",
      "Iter-1429, training loss1.82\n",
      "Iter-1430, training loss1.80\n",
      "Iter-1431, training loss1.75\n",
      "Iter-1432, training loss1.85\n",
      "Iter-1433, training loss1.85\n",
      "Iter-1434, training loss1.77\n",
      "Iter-1435, training loss1.80\n",
      "Iter-1436, training loss1.96\n",
      "Iter-1437, training loss1.77\n",
      "Iter-1438, training loss1.79\n",
      "Iter-1439, training loss1.86\n",
      "Iter-1440, training loss1.77\n",
      "Iter-1441, training loss1.74\n",
      "Iter-1442, training loss1.83\n",
      "Iter-1443, training loss1.72\n",
      "Iter-1444, training loss1.70\n",
      "Iter-1445, training loss1.90\n",
      "Iter-1446, training loss1.89\n",
      "Iter-1447, training loss1.89\n",
      "Iter-1448, training loss1.91\n",
      "Iter-1449, training loss1.79\n",
      "Iter-1450, training loss1.80\n",
      "Iter-1451, training loss1.78\n",
      "Iter-1452, training loss1.79\n",
      "Iter-1453, training loss1.65\n",
      "Iter-1454, training loss1.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1455, training loss1.79\n",
      "Iter-1456, training loss1.93\n",
      "Iter-1457, training loss1.70\n",
      "Iter-1458, training loss1.87\n",
      "Iter-1459, training loss1.79\n",
      "Iter-1460, training loss1.80\n",
      "Iter-1461, training loss1.85\n",
      "Iter-1462, training loss1.84\n",
      "Iter-1463, training loss1.65\n",
      "Iter-1464, training loss1.79\n",
      "Iter-1465, training loss1.91\n",
      "Iter-1466, training loss1.85\n",
      "Iter-1467, training loss1.83\n",
      "Iter-1468, training loss1.90\n",
      "Iter-1469, training loss1.85\n",
      "Iter-1470, training loss1.77\n",
      "Iter-1471, training loss1.94\n",
      "Iter-1472, training loss1.74\n",
      "Iter-1473, training loss1.82\n",
      "Iter-1474, training loss1.85\n",
      "Iter-1475, training loss1.89\n",
      "Iter-1476, training loss1.76\n",
      "Iter-1477, training loss1.76\n",
      "Iter-1478, training loss1.71\n",
      "Iter-1479, training loss1.80\n",
      "Iter-1480, training loss1.83\n",
      "Iter-1481, training loss1.75\n",
      "Iter-1482, training loss1.71\n",
      "Iter-1483, training loss1.98\n",
      "Iter-1484, training loss1.89\n",
      "Iter-1485, training loss1.77\n",
      "Iter-1486, training loss1.88\n",
      "Iter-1487, training loss1.76\n",
      "Iter-1488, training loss1.77\n",
      "Iter-1489, training loss1.86\n",
      "Iter-1490, training loss1.85\n",
      "Iter-1491, training loss1.76\n",
      "Iter-1492, training loss1.83\n",
      "Iter-1493, training loss1.84\n",
      "Iter-1494, training loss1.71\n",
      "Iter-1495, training loss1.79\n",
      "Iter-1496, training loss1.77\n",
      "Iter-1497, training loss1.78\n",
      "Iter-1498, training loss1.82\n",
      "Iter-1499, training loss1.86\n",
      "Iter-1500, training loss1.91\n",
      "Epoch-4\n",
      "-----------------\n",
      "Iter-1501, training loss1.77\n",
      "Iter-1502, training loss1.92\n",
      "Iter-1503, training loss1.79\n",
      "Iter-1504, training loss1.85\n",
      "Iter-1505, training loss1.97\n",
      "Iter-1506, training loss1.86\n",
      "Iter-1507, training loss1.66\n",
      "Iter-1508, training loss1.68\n",
      "Iter-1509, training loss1.84\n",
      "Iter-1510, training loss1.62\n",
      "Iter-1511, training loss1.86\n",
      "Iter-1512, training loss1.86\n",
      "Iter-1513, training loss1.87\n",
      "Iter-1514, training loss1.79\n",
      "Iter-1515, training loss1.97\n",
      "Iter-1516, training loss1.73\n",
      "Iter-1517, training loss1.85\n",
      "Iter-1518, training loss1.76\n",
      "Iter-1519, training loss1.85\n",
      "Iter-1520, training loss1.82\n",
      "Iter-1521, training loss1.82\n",
      "Iter-1522, training loss1.84\n",
      "Iter-1523, training loss1.91\n",
      "Iter-1524, training loss1.72\n",
      "Iter-1525, training loss1.88\n",
      "Iter-1526, training loss1.78\n",
      "Iter-1527, training loss1.90\n",
      "Iter-1528, training loss1.88\n",
      "Iter-1529, training loss1.87\n",
      "Iter-1530, training loss1.69\n",
      "Iter-1531, training loss1.73\n",
      "Iter-1532, training loss1.92\n",
      "Iter-1533, training loss1.89\n",
      "Iter-1534, training loss1.82\n",
      "Iter-1535, training loss1.87\n",
      "Iter-1536, training loss1.79\n",
      "Iter-1537, training loss1.68\n",
      "Iter-1538, training loss1.81\n",
      "Iter-1539, training loss1.74\n",
      "Iter-1540, training loss1.80\n",
      "Iter-1541, training loss1.79\n",
      "Iter-1542, training loss1.81\n",
      "Iter-1543, training loss1.83\n",
      "Iter-1544, training loss1.78\n",
      "Iter-1545, training loss1.92\n",
      "Iter-1546, training loss1.72\n",
      "Iter-1547, training loss1.79\n",
      "Iter-1548, training loss1.76\n",
      "Iter-1549, training loss1.92\n",
      "Iter-1550, training loss1.72\n",
      "Iter-1551, training loss1.98\n",
      "Iter-1552, training loss1.71\n",
      "Iter-1553, training loss2.00\n",
      "Iter-1554, training loss1.91\n",
      "Iter-1555, training loss1.81\n",
      "Iter-1556, training loss1.77\n",
      "Iter-1557, training loss1.81\n",
      "Iter-1558, training loss1.81\n",
      "Iter-1559, training loss1.94\n",
      "Iter-1560, training loss1.90\n",
      "Iter-1561, training loss1.72\n",
      "Iter-1562, training loss1.91\n",
      "Iter-1563, training loss1.75\n",
      "Iter-1564, training loss1.84\n",
      "Iter-1565, training loss1.67\n",
      "Iter-1566, training loss1.74\n",
      "Iter-1567, training loss1.86\n",
      "Iter-1568, training loss2.10\n",
      "Iter-1569, training loss1.93\n",
      "Iter-1570, training loss1.71\n",
      "Iter-1571, training loss1.76\n",
      "Iter-1572, training loss1.82\n",
      "Iter-1573, training loss1.91\n",
      "Iter-1574, training loss1.83\n",
      "Iter-1575, training loss1.85\n",
      "Iter-1576, training loss1.77\n",
      "Iter-1577, training loss1.77\n",
      "Iter-1578, training loss1.79\n",
      "Iter-1579, training loss1.86\n",
      "Iter-1580, training loss1.87\n",
      "Iter-1581, training loss1.94\n",
      "Iter-1582, training loss1.85\n",
      "Iter-1583, training loss1.82\n",
      "Iter-1584, training loss1.69\n",
      "Iter-1585, training loss1.84\n",
      "Iter-1586, training loss1.77\n",
      "Iter-1587, training loss1.67\n",
      "Iter-1588, training loss1.71\n",
      "Iter-1589, training loss1.94\n",
      "Iter-1590, training loss1.65\n",
      "Iter-1591, training loss1.77\n",
      "Iter-1592, training loss1.82\n",
      "Iter-1593, training loss1.88\n",
      "Iter-1594, training loss1.88\n",
      "Iter-1595, training loss1.84\n",
      "Iter-1596, training loss1.73\n",
      "Iter-1597, training loss1.73\n",
      "Iter-1598, training loss1.87\n",
      "Iter-1599, training loss1.85\n",
      "Iter-1600, training loss1.76\n",
      "Iter-1601, training loss1.83\n",
      "Iter-1602, training loss1.91\n",
      "Iter-1603, training loss1.88\n",
      "Iter-1604, training loss1.71\n",
      "Iter-1605, training loss1.83\n",
      "Iter-1606, training loss1.84\n",
      "Iter-1607, training loss1.73\n",
      "Iter-1608, training loss1.73\n",
      "Iter-1609, training loss1.93\n",
      "Iter-1610, training loss1.79\n",
      "Iter-1611, training loss1.83\n",
      "Iter-1612, training loss1.75\n",
      "Iter-1613, training loss1.88\n",
      "Iter-1614, training loss1.93\n",
      "Iter-1615, training loss1.80\n",
      "Iter-1616, training loss1.66\n",
      "Iter-1617, training loss1.72\n",
      "Iter-1618, training loss1.82\n",
      "Iter-1619, training loss1.83\n",
      "Iter-1620, training loss1.83\n",
      "Iter-1621, training loss1.73\n",
      "Iter-1622, training loss1.88\n",
      "Iter-1623, training loss1.82\n",
      "Iter-1624, training loss1.84\n",
      "Iter-1625, training loss1.86\n",
      "Iter-1626, training loss1.91\n",
      "Iter-1627, training loss1.91\n",
      "Iter-1628, training loss1.74\n",
      "Iter-1629, training loss1.63\n",
      "Iter-1630, training loss1.94\n",
      "Iter-1631, training loss1.87\n",
      "Iter-1632, training loss1.76\n",
      "Iter-1633, training loss1.74\n",
      "Iter-1634, training loss1.78\n",
      "Iter-1635, training loss1.96\n",
      "Iter-1636, training loss1.75\n",
      "Iter-1637, training loss1.92\n",
      "Iter-1638, training loss1.83\n",
      "Iter-1639, training loss1.94\n",
      "Iter-1640, training loss1.84\n",
      "Iter-1641, training loss1.74\n",
      "Iter-1642, training loss1.78\n",
      "Iter-1643, training loss1.84\n",
      "Iter-1644, training loss1.70\n",
      "Iter-1645, training loss1.70\n",
      "Iter-1646, training loss1.72\n",
      "Iter-1647, training loss1.83\n",
      "Iter-1648, training loss1.74\n",
      "Iter-1649, training loss1.73\n",
      "Iter-1650, training loss1.80\n",
      "Iter-1651, training loss1.76\n",
      "Iter-1652, training loss1.84\n",
      "Iter-1653, training loss1.80\n",
      "Iter-1654, training loss1.73\n",
      "Iter-1655, training loss1.75\n",
      "Iter-1656, training loss2.10\n",
      "Iter-1657, training loss1.87\n",
      "Iter-1658, training loss1.82\n",
      "Iter-1659, training loss1.82\n",
      "Iter-1660, training loss1.82\n",
      "Iter-1661, training loss1.89\n",
      "Iter-1662, training loss1.69\n",
      "Iter-1663, training loss1.95\n",
      "Iter-1664, training loss1.91\n",
      "Iter-1665, training loss1.69\n",
      "Iter-1666, training loss1.88\n",
      "Iter-1667, training loss1.85\n",
      "Iter-1668, training loss1.90\n",
      "Iter-1669, training loss1.66\n",
      "Iter-1670, training loss1.86\n",
      "Iter-1671, training loss1.75\n",
      "Iter-1672, training loss1.71\n",
      "Iter-1673, training loss1.65\n",
      "Iter-1674, training loss1.76\n",
      "Iter-1675, training loss1.90\n",
      "Iter-1676, training loss1.93\n",
      "Iter-1677, training loss1.87\n",
      "Iter-1678, training loss1.79\n",
      "Iter-1679, training loss1.97\n",
      "Iter-1680, training loss1.91\n",
      "Iter-1681, training loss1.78\n",
      "Iter-1682, training loss1.78\n",
      "Iter-1683, training loss1.82\n",
      "Iter-1684, training loss1.81\n",
      "Iter-1685, training loss2.01\n",
      "Iter-1686, training loss1.63\n",
      "Iter-1687, training loss1.81\n",
      "Iter-1688, training loss1.91\n",
      "Iter-1689, training loss1.83\n",
      "Iter-1690, training loss1.65\n",
      "Iter-1691, training loss1.78\n",
      "Iter-1692, training loss1.85\n",
      "Iter-1693, training loss1.80\n",
      "Iter-1694, training loss1.86\n",
      "Iter-1695, training loss1.79\n",
      "Iter-1696, training loss1.68\n",
      "Iter-1697, training loss1.77\n",
      "Iter-1698, training loss1.87\n",
      "Iter-1699, training loss1.89\n",
      "Iter-1700, training loss1.79\n",
      "Iter-1701, training loss1.99\n",
      "Iter-1702, training loss1.92\n",
      "Iter-1703, training loss1.83\n",
      "Iter-1704, training loss1.65\n",
      "Iter-1705, training loss1.68\n",
      "Iter-1706, training loss1.77\n",
      "Iter-1707, training loss1.69\n",
      "Iter-1708, training loss1.90\n",
      "Iter-1709, training loss1.69\n",
      "Iter-1710, training loss1.85\n",
      "Iter-1711, training loss1.98\n",
      "Iter-1712, training loss1.72\n",
      "Iter-1713, training loss1.83\n",
      "Iter-1714, training loss1.79\n",
      "Iter-1715, training loss1.72\n",
      "Iter-1716, training loss1.65\n",
      "Iter-1717, training loss1.71\n",
      "Iter-1718, training loss1.87\n",
      "Iter-1719, training loss1.85\n",
      "Iter-1720, training loss1.81\n",
      "Iter-1721, training loss1.91\n",
      "Iter-1722, training loss1.91\n",
      "Iter-1723, training loss1.73\n",
      "Iter-1724, training loss1.90\n",
      "Iter-1725, training loss1.81\n",
      "Iter-1726, training loss1.82\n",
      "Iter-1727, training loss1.66\n",
      "Iter-1728, training loss1.85\n",
      "Iter-1729, training loss1.88\n",
      "Iter-1730, training loss1.79\n",
      "Iter-1731, training loss1.76\n",
      "Iter-1732, training loss1.94\n",
      "Iter-1733, training loss1.91\n",
      "Iter-1734, training loss1.77\n",
      "Iter-1735, training loss1.81\n",
      "Iter-1736, training loss2.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1737, training loss1.75\n",
      "Iter-1738, training loss1.84\n",
      "Iter-1739, training loss1.60\n",
      "Iter-1740, training loss1.67\n",
      "Iter-1741, training loss1.80\n",
      "Iter-1742, training loss1.67\n",
      "Iter-1743, training loss1.70\n",
      "Iter-1744, training loss1.96\n",
      "Iter-1745, training loss1.84\n",
      "Iter-1746, training loss1.70\n",
      "Iter-1747, training loss1.93\n",
      "Iter-1748, training loss1.73\n",
      "Iter-1749, training loss1.87\n",
      "Iter-1750, training loss1.76\n",
      "Iter-1751, training loss1.75\n",
      "Iter-1752, training loss1.84\n",
      "Iter-1753, training loss1.90\n",
      "Iter-1754, training loss1.74\n",
      "Iter-1755, training loss1.99\n",
      "Iter-1756, training loss1.88\n",
      "Iter-1757, training loss1.75\n",
      "Iter-1758, training loss1.71\n",
      "Iter-1759, training loss1.85\n",
      "Iter-1760, training loss1.89\n",
      "Iter-1761, training loss1.74\n",
      "Iter-1762, training loss1.68\n",
      "Iter-1763, training loss1.75\n",
      "Iter-1764, training loss1.78\n",
      "Iter-1765, training loss1.83\n",
      "Iter-1766, training loss1.89\n",
      "Iter-1767, training loss1.69\n",
      "Iter-1768, training loss1.89\n",
      "Iter-1769, training loss1.89\n",
      "Iter-1770, training loss1.76\n",
      "Iter-1771, training loss1.82\n",
      "Iter-1772, training loss1.70\n",
      "Iter-1773, training loss1.77\n",
      "Iter-1774, training loss1.70\n",
      "Iter-1775, training loss1.70\n",
      "Iter-1776, training loss1.89\n",
      "Iter-1777, training loss1.97\n",
      "Iter-1778, training loss1.76\n",
      "Iter-1779, training loss1.94\n",
      "Iter-1780, training loss1.92\n",
      "Iter-1781, training loss1.76\n",
      "Iter-1782, training loss1.71\n",
      "Iter-1783, training loss1.79\n",
      "Iter-1784, training loss1.66\n",
      "Iter-1785, training loss1.72\n",
      "Iter-1786, training loss1.79\n",
      "Iter-1787, training loss1.80\n",
      "Iter-1788, training loss1.72\n",
      "Iter-1789, training loss1.76\n",
      "Iter-1790, training loss1.90\n",
      "Iter-1791, training loss1.76\n",
      "Iter-1792, training loss1.87\n",
      "Iter-1793, training loss1.90\n",
      "Iter-1794, training loss1.78\n",
      "Iter-1795, training loss1.80\n",
      "Iter-1796, training loss1.74\n",
      "Iter-1797, training loss1.90\n",
      "Iter-1798, training loss1.87\n",
      "Iter-1799, training loss1.70\n",
      "Iter-1800, training loss1.73\n",
      "Iter-1801, training loss1.78\n",
      "Iter-1802, training loss1.90\n",
      "Iter-1803, training loss1.69\n",
      "Iter-1804, training loss1.82\n",
      "Iter-1805, training loss1.90\n",
      "Iter-1806, training loss1.78\n",
      "Iter-1807, training loss1.74\n",
      "Iter-1808, training loss1.82\n",
      "Iter-1809, training loss1.76\n",
      "Iter-1810, training loss1.84\n",
      "Iter-1811, training loss1.79\n",
      "Iter-1812, training loss1.85\n",
      "Iter-1813, training loss1.73\n",
      "Iter-1814, training loss1.84\n",
      "Iter-1815, training loss1.82\n",
      "Iter-1816, training loss1.89\n",
      "Iter-1817, training loss1.80\n",
      "Iter-1818, training loss1.85\n",
      "Iter-1819, training loss1.87\n",
      "Iter-1820, training loss1.88\n",
      "Iter-1821, training loss1.75\n",
      "Iter-1822, training loss1.77\n",
      "Iter-1823, training loss1.75\n",
      "Iter-1824, training loss1.78\n",
      "Iter-1825, training loss1.69\n",
      "Iter-1826, training loss1.85\n",
      "Iter-1827, training loss1.75\n",
      "Iter-1828, training loss1.81\n",
      "Iter-1829, training loss1.94\n",
      "Iter-1830, training loss1.77\n",
      "Iter-1831, training loss1.66\n",
      "Iter-1832, training loss1.87\n",
      "Iter-1833, training loss1.77\n",
      "Iter-1834, training loss1.66\n",
      "Iter-1835, training loss1.80\n",
      "Iter-1836, training loss1.78\n",
      "Iter-1837, training loss1.71\n",
      "Iter-1838, training loss1.71\n",
      "Iter-1839, training loss1.73\n",
      "Iter-1840, training loss1.85\n",
      "Iter-1841, training loss1.74\n",
      "Iter-1842, training loss1.86\n",
      "Iter-1843, training loss1.70\n",
      "Iter-1844, training loss1.63\n",
      "Iter-1845, training loss1.87\n",
      "Iter-1846, training loss1.76\n",
      "Iter-1847, training loss1.82\n",
      "Iter-1848, training loss1.86\n",
      "Iter-1849, training loss1.83\n",
      "Iter-1850, training loss1.79\n",
      "Iter-1851, training loss1.66\n",
      "Iter-1852, training loss1.91\n",
      "Iter-1853, training loss1.87\n",
      "Iter-1854, training loss1.86\n",
      "Iter-1855, training loss1.75\n",
      "Iter-1856, training loss1.70\n",
      "Iter-1857, training loss1.69\n",
      "Iter-1858, training loss1.84\n",
      "Iter-1859, training loss1.84\n",
      "Iter-1860, training loss1.78\n",
      "Iter-1861, training loss1.67\n",
      "Iter-1862, training loss1.71\n",
      "Iter-1863, training loss1.79\n",
      "Iter-1864, training loss1.89\n",
      "Iter-1865, training loss1.79\n",
      "Iter-1866, training loss1.84\n",
      "Iter-1867, training loss1.83\n",
      "Iter-1868, training loss1.78\n",
      "Iter-1869, training loss1.67\n",
      "Iter-1870, training loss1.79\n",
      "Iter-1871, training loss1.70\n",
      "Iter-1872, training loss1.63\n",
      "Iter-1873, training loss1.83\n",
      "Iter-1874, training loss1.87\n",
      "Iter-1875, training loss1.92\n",
      "Iter-1876, training loss1.79\n",
      "Iter-1877, training loss1.87\n",
      "Iter-1878, training loss1.73\n",
      "Iter-1879, training loss1.83\n",
      "Iter-1880, training loss1.90\n",
      "Iter-1881, training loss1.78\n",
      "Iter-1882, training loss1.75\n",
      "Iter-1883, training loss1.83\n",
      "Iter-1884, training loss1.75\n",
      "Iter-1885, training loss1.78\n",
      "Iter-1886, training loss1.71\n",
      "Iter-1887, training loss1.84\n",
      "Iter-1888, training loss1.69\n",
      "Iter-1889, training loss1.85\n",
      "Iter-1890, training loss1.86\n",
      "Iter-1891, training loss1.69\n",
      "Iter-1892, training loss1.69\n",
      "Iter-1893, training loss1.72\n",
      "Iter-1894, training loss1.68\n",
      "Iter-1895, training loss1.89\n",
      "Iter-1896, training loss1.77\n",
      "Iter-1897, training loss1.76\n",
      "Iter-1898, training loss1.84\n",
      "Iter-1899, training loss1.72\n",
      "Iter-1900, training loss1.87\n",
      "Iter-1901, training loss1.75\n",
      "Iter-1902, training loss1.79\n",
      "Iter-1903, training loss1.83\n",
      "Iter-1904, training loss1.79\n",
      "Iter-1905, training loss1.86\n",
      "Iter-1906, training loss1.74\n",
      "Iter-1907, training loss1.76\n",
      "Iter-1908, training loss1.84\n",
      "Iter-1909, training loss1.65\n",
      "Iter-1910, training loss1.77\n",
      "Iter-1911, training loss1.87\n",
      "Iter-1912, training loss1.74\n",
      "Iter-1913, training loss1.87\n",
      "Iter-1914, training loss1.78\n",
      "Iter-1915, training loss1.77\n",
      "Iter-1916, training loss1.93\n",
      "Iter-1917, training loss1.82\n",
      "Iter-1918, training loss1.74\n",
      "Iter-1919, training loss1.81\n",
      "Iter-1920, training loss1.71\n",
      "Iter-1921, training loss1.64\n",
      "Iter-1922, training loss1.87\n",
      "Iter-1923, training loss1.80\n",
      "Iter-1924, training loss1.78\n",
      "Iter-1925, training loss1.74\n",
      "Iter-1926, training loss1.76\n",
      "Iter-1927, training loss1.84\n",
      "Iter-1928, training loss1.68\n",
      "Iter-1929, training loss1.87\n",
      "Iter-1930, training loss1.73\n",
      "Iter-1931, training loss1.83\n",
      "Iter-1932, training loss1.87\n",
      "Iter-1933, training loss1.79\n",
      "Iter-1934, training loss1.88\n",
      "Iter-1935, training loss1.76\n",
      "Iter-1936, training loss1.78\n",
      "Iter-1937, training loss1.78\n",
      "Iter-1938, training loss1.77\n",
      "Iter-1939, training loss1.83\n",
      "Iter-1940, training loss1.85\n",
      "Iter-1941, training loss1.79\n",
      "Iter-1942, training loss1.60\n",
      "Iter-1943, training loss1.67\n",
      "Iter-1944, training loss1.61\n",
      "Iter-1945, training loss1.77\n",
      "Iter-1946, training loss1.80\n",
      "Iter-1947, training loss1.66\n",
      "Iter-1948, training loss1.82\n",
      "Iter-1949, training loss1.88\n",
      "Iter-1950, training loss1.71\n",
      "Iter-1951, training loss1.98\n",
      "Iter-1952, training loss1.65\n",
      "Iter-1953, training loss1.88\n",
      "Iter-1954, training loss1.70\n",
      "Iter-1955, training loss1.80\n",
      "Iter-1956, training loss1.72\n",
      "Iter-1957, training loss1.78\n",
      "Iter-1958, training loss1.85\n",
      "Iter-1959, training loss1.92\n",
      "Iter-1960, training loss1.77\n",
      "Iter-1961, training loss1.71\n",
      "Iter-1962, training loss1.73\n",
      "Iter-1963, training loss1.71\n",
      "Iter-1964, training loss1.81\n",
      "Iter-1965, training loss1.71\n",
      "Iter-1966, training loss1.94\n",
      "Iter-1967, training loss1.92\n",
      "Iter-1968, training loss1.74\n",
      "Iter-1969, training loss1.71\n",
      "Iter-1970, training loss1.81\n",
      "Iter-1971, training loss1.92\n",
      "Iter-1972, training loss1.88\n",
      "Iter-1973, training loss1.86\n",
      "Iter-1974, training loss1.89\n",
      "Iter-1975, training loss1.70\n",
      "Iter-1976, training loss1.81\n",
      "Iter-1977, training loss1.81\n",
      "Iter-1978, training loss1.64\n",
      "Iter-1979, training loss1.74\n",
      "Iter-1980, training loss1.87\n",
      "Iter-1981, training loss1.89\n",
      "Iter-1982, training loss1.76\n",
      "Iter-1983, training loss1.67\n",
      "Iter-1984, training loss2.00\n",
      "Iter-1985, training loss1.90\n",
      "Iter-1986, training loss1.69\n",
      "Iter-1987, training loss1.76\n",
      "Iter-1988, training loss1.78\n",
      "Iter-1989, training loss1.90\n",
      "Iter-1990, training loss1.81\n",
      "Iter-1991, training loss1.62\n",
      "Iter-1992, training loss1.70\n",
      "Iter-1993, training loss1.89\n",
      "Iter-1994, training loss1.64\n",
      "Iter-1995, training loss1.84\n",
      "Iter-1996, training loss1.61\n",
      "Iter-1997, training loss1.70\n",
      "Iter-1998, training loss1.79\n",
      "Iter-1999, training loss1.69\n",
      "Iter-2000, training loss1.94\n",
      "Epoch-5\n",
      "-----------------\n",
      "Iter-2001, training loss1.77\n",
      "Iter-2002, training loss1.69\n",
      "Iter-2003, training loss1.67\n",
      "Iter-2004, training loss1.76\n",
      "Iter-2005, training loss1.90\n",
      "Iter-2006, training loss1.66\n",
      "Iter-2007, training loss1.88\n",
      "Iter-2008, training loss1.75\n",
      "Iter-2009, training loss1.75\n",
      "Iter-2010, training loss1.65\n",
      "Iter-2011, training loss1.73\n",
      "Iter-2012, training loss1.71\n",
      "Iter-2013, training loss1.75\n",
      "Iter-2014, training loss1.78\n",
      "Iter-2015, training loss1.77\n",
      "Iter-2016, training loss1.88\n",
      "Iter-2017, training loss1.65\n",
      "Iter-2018, training loss1.69\n",
      "Iter-2019, training loss1.70\n",
      "Iter-2020, training loss1.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2021, training loss1.72\n",
      "Iter-2022, training loss1.75\n",
      "Iter-2023, training loss1.80\n",
      "Iter-2024, training loss1.68\n",
      "Iter-2025, training loss1.64\n",
      "Iter-2026, training loss2.00\n",
      "Iter-2027, training loss1.92\n",
      "Iter-2028, training loss1.85\n",
      "Iter-2029, training loss1.82\n",
      "Iter-2030, training loss1.86\n",
      "Iter-2031, training loss1.91\n",
      "Iter-2032, training loss1.53\n",
      "Iter-2033, training loss1.86\n",
      "Iter-2034, training loss1.82\n",
      "Iter-2035, training loss1.83\n",
      "Iter-2036, training loss1.78\n",
      "Iter-2037, training loss1.78\n",
      "Iter-2038, training loss1.91\n",
      "Iter-2039, training loss1.82\n",
      "Iter-2040, training loss1.82\n",
      "Iter-2041, training loss1.75\n",
      "Iter-2042, training loss1.74\n",
      "Iter-2043, training loss1.59\n",
      "Iter-2044, training loss1.72\n",
      "Iter-2045, training loss1.84\n",
      "Iter-2046, training loss1.87\n",
      "Iter-2047, training loss1.67\n",
      "Iter-2048, training loss2.03\n",
      "Iter-2049, training loss1.92\n",
      "Iter-2050, training loss1.71\n",
      "Iter-2051, training loss1.93\n",
      "Iter-2052, training loss1.81\n",
      "Iter-2053, training loss1.79\n",
      "Iter-2054, training loss1.77\n",
      "Iter-2055, training loss1.83\n",
      "Iter-2056, training loss1.64\n",
      "Iter-2057, training loss1.83\n",
      "Iter-2058, training loss1.73\n",
      "Iter-2059, training loss1.88\n",
      "Iter-2060, training loss1.90\n",
      "Iter-2061, training loss1.61\n",
      "Iter-2062, training loss1.88\n",
      "Iter-2063, training loss1.69\n",
      "Iter-2064, training loss1.79\n",
      "Iter-2065, training loss1.84\n",
      "Iter-2066, training loss1.88\n",
      "Iter-2067, training loss1.79\n",
      "Iter-2068, training loss1.84\n",
      "Iter-2069, training loss1.74\n",
      "Iter-2070, training loss1.85\n",
      "Iter-2071, training loss1.93\n",
      "Iter-2072, training loss1.83\n",
      "Iter-2073, training loss1.89\n",
      "Iter-2074, training loss1.76\n",
      "Iter-2075, training loss1.86\n",
      "Iter-2076, training loss1.62\n",
      "Iter-2077, training loss1.77\n",
      "Iter-2078, training loss1.74\n",
      "Iter-2079, training loss1.89\n",
      "Iter-2080, training loss1.73\n",
      "Iter-2081, training loss1.71\n",
      "Iter-2082, training loss1.76\n",
      "Iter-2083, training loss1.79\n",
      "Iter-2084, training loss1.81\n",
      "Iter-2085, training loss1.66\n",
      "Iter-2086, training loss1.78\n",
      "Iter-2087, training loss1.76\n",
      "Iter-2088, training loss1.88\n",
      "Iter-2089, training loss1.81\n",
      "Iter-2090, training loss1.75\n",
      "Iter-2091, training loss1.94\n",
      "Iter-2092, training loss1.74\n",
      "Iter-2093, training loss1.76\n",
      "Iter-2094, training loss1.63\n",
      "Iter-2095, training loss1.79\n",
      "Iter-2096, training loss1.76\n",
      "Iter-2097, training loss1.73\n",
      "Iter-2098, training loss1.90\n",
      "Iter-2099, training loss1.70\n",
      "Iter-2100, training loss1.81\n",
      "Iter-2101, training loss1.73\n",
      "Iter-2102, training loss1.82\n",
      "Iter-2103, training loss1.83\n",
      "Iter-2104, training loss1.78\n",
      "Iter-2105, training loss1.70\n",
      "Iter-2106, training loss1.60\n",
      "Iter-2107, training loss1.86\n",
      "Iter-2108, training loss1.81\n",
      "Iter-2109, training loss1.89\n",
      "Iter-2110, training loss1.76\n",
      "Iter-2111, training loss1.75\n",
      "Iter-2112, training loss1.79\n",
      "Iter-2113, training loss1.77\n",
      "Iter-2114, training loss1.71\n",
      "Iter-2115, training loss1.90\n",
      "Iter-2116, training loss1.90\n",
      "Iter-2117, training loss1.72\n",
      "Iter-2118, training loss1.77\n",
      "Iter-2119, training loss1.76\n",
      "Iter-2120, training loss1.82\n",
      "Iter-2121, training loss1.79\n",
      "Iter-2122, training loss1.75\n",
      "Iter-2123, training loss1.96\n",
      "Iter-2124, training loss1.79\n",
      "Iter-2125, training loss1.78\n",
      "Iter-2126, training loss1.89\n",
      "Iter-2127, training loss1.88\n",
      "Iter-2128, training loss1.69\n",
      "Iter-2129, training loss1.74\n",
      "Iter-2130, training loss1.88\n",
      "Iter-2131, training loss1.72\n",
      "Iter-2132, training loss1.63\n",
      "Iter-2133, training loss1.90\n",
      "Iter-2134, training loss1.84\n",
      "Iter-2135, training loss1.82\n",
      "Iter-2136, training loss1.66\n",
      "Iter-2137, training loss1.76\n",
      "Iter-2138, training loss1.80\n",
      "Iter-2139, training loss1.74\n",
      "Iter-2140, training loss1.86\n",
      "Iter-2141, training loss1.79\n",
      "Iter-2142, training loss1.75\n",
      "Iter-2143, training loss1.67\n",
      "Iter-2144, training loss1.63\n",
      "Iter-2145, training loss1.62\n",
      "Iter-2146, training loss1.86\n",
      "Iter-2147, training loss1.81\n",
      "Iter-2148, training loss1.91\n",
      "Iter-2149, training loss1.95\n",
      "Iter-2150, training loss1.81\n",
      "Iter-2151, training loss1.74\n",
      "Iter-2152, training loss1.78\n",
      "Iter-2153, training loss1.86\n",
      "Iter-2154, training loss1.78\n",
      "Iter-2155, training loss1.75\n",
      "Iter-2156, training loss1.87\n",
      "Iter-2157, training loss1.71\n",
      "Iter-2158, training loss1.84\n",
      "Iter-2159, training loss1.65\n",
      "Iter-2160, training loss1.78\n",
      "Iter-2161, training loss1.87\n",
      "Iter-2162, training loss1.86\n",
      "Iter-2163, training loss1.68\n",
      "Iter-2164, training loss1.73\n",
      "Iter-2165, training loss1.94\n",
      "Iter-2166, training loss1.83\n",
      "Iter-2167, training loss1.75\n",
      "Iter-2168, training loss1.93\n",
      "Iter-2169, training loss1.73\n",
      "Iter-2170, training loss1.75\n",
      "Iter-2171, training loss1.78\n",
      "Iter-2172, training loss1.91\n",
      "Iter-2173, training loss1.71\n",
      "Iter-2174, training loss1.77\n",
      "Iter-2175, training loss1.78\n",
      "Iter-2176, training loss1.76\n",
      "Iter-2177, training loss1.88\n",
      "Iter-2178, training loss1.77\n",
      "Iter-2179, training loss1.79\n",
      "Iter-2180, training loss1.69\n",
      "Iter-2181, training loss1.70\n",
      "Iter-2182, training loss1.91\n",
      "Iter-2183, training loss1.68\n",
      "Iter-2184, training loss1.81\n",
      "Iter-2185, training loss1.70\n",
      "Iter-2186, training loss1.76\n",
      "Iter-2187, training loss1.49\n",
      "Iter-2188, training loss1.72\n",
      "Iter-2189, training loss1.82\n",
      "Iter-2190, training loss1.73\n",
      "Iter-2191, training loss1.60\n",
      "Iter-2192, training loss1.94\n",
      "Iter-2193, training loss1.74\n",
      "Iter-2194, training loss1.91\n",
      "Iter-2195, training loss1.69\n",
      "Iter-2196, training loss1.77\n",
      "Iter-2197, training loss1.87\n",
      "Iter-2198, training loss1.80\n",
      "Iter-2199, training loss1.70\n",
      "Iter-2200, training loss1.81\n",
      "Iter-2201, training loss1.82\n",
      "Iter-2202, training loss1.84\n",
      "Iter-2203, training loss1.75\n",
      "Iter-2204, training loss1.74\n",
      "Iter-2205, training loss1.76\n",
      "Iter-2206, training loss1.80\n",
      "Iter-2207, training loss1.85\n",
      "Iter-2208, training loss1.63\n",
      "Iter-2209, training loss1.83\n",
      "Iter-2210, training loss1.76\n",
      "Iter-2211, training loss1.84\n",
      "Iter-2212, training loss1.68\n",
      "Iter-2213, training loss1.69\n",
      "Iter-2214, training loss1.66\n",
      "Iter-2215, training loss1.76\n",
      "Iter-2216, training loss1.83\n",
      "Iter-2217, training loss1.72\n",
      "Iter-2218, training loss1.89\n",
      "Iter-2219, training loss1.79\n",
      "Iter-2220, training loss1.94\n",
      "Iter-2221, training loss1.71\n",
      "Iter-2222, training loss1.67\n",
      "Iter-2223, training loss1.69\n",
      "Iter-2224, training loss1.66\n",
      "Iter-2225, training loss1.87\n",
      "Iter-2226, training loss1.65\n",
      "Iter-2227, training loss1.61\n",
      "Iter-2228, training loss1.77\n",
      "Iter-2229, training loss1.77\n",
      "Iter-2230, training loss1.84\n",
      "Iter-2231, training loss1.89\n",
      "Iter-2232, training loss1.70\n",
      "Iter-2233, training loss1.75\n",
      "Iter-2234, training loss1.73\n",
      "Iter-2235, training loss1.85\n",
      "Iter-2236, training loss1.84\n",
      "Iter-2237, training loss1.90\n",
      "Iter-2238, training loss1.79\n",
      "Iter-2239, training loss1.78\n",
      "Iter-2240, training loss1.72\n",
      "Iter-2241, training loss1.78\n",
      "Iter-2242, training loss1.96\n",
      "Iter-2243, training loss1.68\n",
      "Iter-2244, training loss1.81\n",
      "Iter-2245, training loss1.76\n",
      "Iter-2246, training loss1.62\n",
      "Iter-2247, training loss1.84\n",
      "Iter-2248, training loss1.64\n",
      "Iter-2249, training loss1.79\n",
      "Iter-2250, training loss1.69\n",
      "Iter-2251, training loss1.70\n",
      "Iter-2252, training loss1.61\n",
      "Iter-2253, training loss1.78\n",
      "Iter-2254, training loss1.71\n",
      "Iter-2255, training loss1.88\n",
      "Iter-2256, training loss1.71\n",
      "Iter-2257, training loss1.67\n",
      "Iter-2258, training loss1.64\n",
      "Iter-2259, training loss2.05\n",
      "Iter-2260, training loss1.84\n",
      "Iter-2261, training loss1.72\n",
      "Iter-2262, training loss1.75\n",
      "Iter-2263, training loss1.94\n",
      "Iter-2264, training loss1.68\n",
      "Iter-2265, training loss1.63\n",
      "Iter-2266, training loss1.71\n",
      "Iter-2267, training loss1.78\n",
      "Iter-2268, training loss1.79\n",
      "Iter-2269, training loss1.77\n",
      "Iter-2270, training loss1.73\n",
      "Iter-2271, training loss1.82\n",
      "Iter-2272, training loss1.70\n",
      "Iter-2273, training loss1.76\n",
      "Iter-2274, training loss1.76\n",
      "Iter-2275, training loss1.86\n",
      "Iter-2276, training loss1.73\n",
      "Iter-2277, training loss1.60\n",
      "Iter-2278, training loss1.87\n",
      "Iter-2279, training loss1.66\n",
      "Iter-2280, training loss1.80\n",
      "Iter-2281, training loss1.64\n",
      "Iter-2282, training loss1.84\n",
      "Iter-2283, training loss1.77\n",
      "Iter-2284, training loss1.67\n",
      "Iter-2285, training loss1.82\n",
      "Iter-2286, training loss1.67\n",
      "Iter-2287, training loss1.66\n",
      "Iter-2288, training loss1.88\n",
      "Iter-2289, training loss1.74\n",
      "Iter-2290, training loss1.73\n",
      "Iter-2291, training loss1.62\n",
      "Iter-2292, training loss1.85\n",
      "Iter-2293, training loss1.75\n",
      "Iter-2294, training loss1.78\n",
      "Iter-2295, training loss1.83\n",
      "Iter-2296, training loss1.72\n",
      "Iter-2297, training loss1.63\n",
      "Iter-2298, training loss1.97\n",
      "Iter-2299, training loss1.67\n",
      "Iter-2300, training loss1.81\n",
      "Iter-2301, training loss1.65\n",
      "Iter-2302, training loss1.82\n",
      "Iter-2303, training loss1.73\n",
      "Iter-2304, training loss1.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2305, training loss1.80\n",
      "Iter-2306, training loss1.78\n",
      "Iter-2307, training loss1.90\n",
      "Iter-2308, training loss1.74\n",
      "Iter-2309, training loss1.66\n",
      "Iter-2310, training loss1.69\n",
      "Iter-2311, training loss1.72\n",
      "Iter-2312, training loss1.75\n",
      "Iter-2313, training loss1.62\n",
      "Iter-2314, training loss1.70\n",
      "Iter-2315, training loss1.87\n",
      "Iter-2316, training loss1.75\n",
      "Iter-2317, training loss1.73\n",
      "Iter-2318, training loss1.82\n",
      "Iter-2319, training loss1.62\n",
      "Iter-2320, training loss1.73\n",
      "Iter-2321, training loss1.75\n",
      "Iter-2322, training loss1.82\n",
      "Iter-2323, training loss1.92\n",
      "Iter-2324, training loss1.58\n",
      "Iter-2325, training loss1.80\n",
      "Iter-2326, training loss1.79\n",
      "Iter-2327, training loss1.74\n",
      "Iter-2328, training loss1.77\n",
      "Iter-2329, training loss1.80\n",
      "Iter-2330, training loss1.69\n",
      "Iter-2331, training loss1.90\n",
      "Iter-2332, training loss1.73\n",
      "Iter-2333, training loss1.49\n",
      "Iter-2334, training loss1.84\n",
      "Iter-2335, training loss1.79\n",
      "Iter-2336, training loss1.74\n",
      "Iter-2337, training loss1.72\n",
      "Iter-2338, training loss1.84\n",
      "Iter-2339, training loss1.80\n",
      "Iter-2340, training loss1.74\n",
      "Iter-2341, training loss1.72\n",
      "Iter-2342, training loss1.74\n",
      "Iter-2343, training loss1.67\n",
      "Iter-2344, training loss1.70\n",
      "Iter-2345, training loss1.66\n",
      "Iter-2346, training loss1.91\n",
      "Iter-2347, training loss1.76\n",
      "Iter-2348, training loss1.83\n",
      "Iter-2349, training loss1.72\n",
      "Iter-2350, training loss1.82\n",
      "Iter-2351, training loss1.90\n",
      "Iter-2352, training loss1.85\n",
      "Iter-2353, training loss1.87\n",
      "Iter-2354, training loss1.95\n",
      "Iter-2355, training loss1.76\n",
      "Iter-2356, training loss1.85\n",
      "Iter-2357, training loss1.73\n",
      "Iter-2358, training loss1.93\n",
      "Iter-2359, training loss1.71\n",
      "Iter-2360, training loss1.81\n",
      "Iter-2361, training loss1.80\n",
      "Iter-2362, training loss1.71\n",
      "Iter-2363, training loss1.68\n",
      "Iter-2364, training loss1.58\n",
      "Iter-2365, training loss1.79\n",
      "Iter-2366, training loss1.94\n",
      "Iter-2367, training loss1.50\n",
      "Iter-2368, training loss1.76\n",
      "Iter-2369, training loss1.59\n",
      "Iter-2370, training loss1.82\n",
      "Iter-2371, training loss1.64\n",
      "Iter-2372, training loss1.78\n",
      "Iter-2373, training loss1.91\n",
      "Iter-2374, training loss1.75\n",
      "Iter-2375, training loss1.62\n",
      "Iter-2376, training loss1.75\n",
      "Iter-2377, training loss1.77\n",
      "Iter-2378, training loss1.78\n",
      "Iter-2379, training loss1.78\n",
      "Iter-2380, training loss1.84\n",
      "Iter-2381, training loss1.77\n",
      "Iter-2382, training loss1.74\n",
      "Iter-2383, training loss1.76\n",
      "Iter-2384, training loss1.69\n",
      "Iter-2385, training loss1.71\n",
      "Iter-2386, training loss1.82\n",
      "Iter-2387, training loss1.86\n",
      "Iter-2388, training loss1.67\n",
      "Iter-2389, training loss1.76\n",
      "Iter-2390, training loss1.68\n",
      "Iter-2391, training loss1.59\n",
      "Iter-2392, training loss1.89\n",
      "Iter-2393, training loss1.78\n",
      "Iter-2394, training loss1.66\n",
      "Iter-2395, training loss1.76\n",
      "Iter-2396, training loss1.88\n",
      "Iter-2397, training loss1.64\n",
      "Iter-2398, training loss1.71\n",
      "Iter-2399, training loss1.67\n",
      "Iter-2400, training loss1.61\n",
      "Iter-2401, training loss1.82\n",
      "Iter-2402, training loss1.77\n",
      "Iter-2403, training loss1.73\n",
      "Iter-2404, training loss1.82\n",
      "Iter-2405, training loss1.57\n",
      "Iter-2406, training loss1.66\n",
      "Iter-2407, training loss1.72\n",
      "Iter-2408, training loss1.80\n",
      "Iter-2409, training loss1.84\n",
      "Iter-2410, training loss1.78\n",
      "Iter-2411, training loss1.86\n",
      "Iter-2412, training loss1.75\n",
      "Iter-2413, training loss1.79\n",
      "Iter-2414, training loss1.78\n",
      "Iter-2415, training loss1.75\n",
      "Iter-2416, training loss1.79\n",
      "Iter-2417, training loss1.80\n",
      "Iter-2418, training loss1.71\n",
      "Iter-2419, training loss1.74\n",
      "Iter-2420, training loss1.74\n",
      "Iter-2421, training loss1.84\n",
      "Iter-2422, training loss1.79\n",
      "Iter-2423, training loss1.63\n",
      "Iter-2424, training loss1.63\n",
      "Iter-2425, training loss1.74\n",
      "Iter-2426, training loss1.81\n",
      "Iter-2427, training loss1.78\n",
      "Iter-2428, training loss1.77\n",
      "Iter-2429, training loss1.85\n",
      "Iter-2430, training loss1.76\n",
      "Iter-2431, training loss1.70\n",
      "Iter-2432, training loss1.77\n",
      "Iter-2433, training loss1.80\n",
      "Iter-2434, training loss1.65\n",
      "Iter-2435, training loss1.82\n",
      "Iter-2436, training loss1.78\n",
      "Iter-2437, training loss1.66\n",
      "Iter-2438, training loss1.70\n",
      "Iter-2439, training loss1.82\n",
      "Iter-2440, training loss1.75\n",
      "Iter-2441, training loss1.72\n",
      "Iter-2442, training loss1.81\n",
      "Iter-2443, training loss1.66\n",
      "Iter-2444, training loss1.54\n",
      "Iter-2445, training loss1.82\n",
      "Iter-2446, training loss1.73\n",
      "Iter-2447, training loss1.73\n",
      "Iter-2448, training loss1.77\n",
      "Iter-2449, training loss1.65\n",
      "Iter-2450, training loss1.72\n",
      "Iter-2451, training loss1.73\n",
      "Iter-2452, training loss1.71\n",
      "Iter-2453, training loss1.64\n",
      "Iter-2454, training loss1.74\n",
      "Iter-2455, training loss1.69\n",
      "Iter-2456, training loss1.72\n",
      "Iter-2457, training loss1.75\n",
      "Iter-2458, training loss1.82\n",
      "Iter-2459, training loss1.75\n",
      "Iter-2460, training loss1.68\n",
      "Iter-2461, training loss1.52\n",
      "Iter-2462, training loss1.83\n",
      "Iter-2463, training loss1.60\n",
      "Iter-2464, training loss1.74\n",
      "Iter-2465, training loss1.66\n",
      "Iter-2466, training loss1.83\n",
      "Iter-2467, training loss1.75\n",
      "Iter-2468, training loss1.88\n",
      "Iter-2469, training loss1.73\n",
      "Iter-2470, training loss1.70\n",
      "Iter-2471, training loss1.70\n",
      "Iter-2472, training loss1.69\n",
      "Iter-2473, training loss1.86\n",
      "Iter-2474, training loss1.71\n",
      "Iter-2475, training loss1.81\n",
      "Iter-2476, training loss1.76\n",
      "Iter-2477, training loss1.74\n",
      "Iter-2478, training loss1.84\n",
      "Iter-2479, training loss1.75\n",
      "Iter-2480, training loss1.80\n",
      "Iter-2481, training loss1.78\n",
      "Iter-2482, training loss1.89\n",
      "Iter-2483, training loss1.89\n",
      "Iter-2484, training loss1.65\n",
      "Iter-2485, training loss1.78\n",
      "Iter-2486, training loss1.69\n",
      "Iter-2487, training loss1.69\n",
      "Iter-2488, training loss1.65\n",
      "Iter-2489, training loss1.96\n",
      "Iter-2490, training loss1.80\n",
      "Iter-2491, training loss1.77\n",
      "Iter-2492, training loss1.72\n",
      "Iter-2493, training loss1.63\n",
      "Iter-2494, training loss1.64\n",
      "Iter-2495, training loss1.73\n",
      "Iter-2496, training loss1.77\n",
      "Iter-2497, training loss1.58\n",
      "Iter-2498, training loss1.63\n",
      "Iter-2499, training loss1.68\n",
      "Iter-2500, training loss1.68\n",
      "Epoch-6\n",
      "-----------------\n",
      "Iter-2501, training loss1.55\n",
      "Iter-2502, training loss1.68\n",
      "Iter-2503, training loss1.88\n",
      "Iter-2504, training loss1.63\n",
      "Iter-2505, training loss1.76\n",
      "Iter-2506, training loss1.66\n",
      "Iter-2507, training loss1.83\n",
      "Iter-2508, training loss1.72\n",
      "Iter-2509, training loss1.68\n",
      "Iter-2510, training loss1.72\n",
      "Iter-2511, training loss1.84\n",
      "Iter-2512, training loss1.81\n",
      "Iter-2513, training loss1.82\n",
      "Iter-2514, training loss1.76\n",
      "Iter-2515, training loss1.77\n",
      "Iter-2516, training loss1.74\n",
      "Iter-2517, training loss1.81\n",
      "Iter-2518, training loss1.81\n",
      "Iter-2519, training loss1.85\n",
      "Iter-2520, training loss1.80\n",
      "Iter-2521, training loss1.83\n",
      "Iter-2522, training loss1.66\n",
      "Iter-2523, training loss1.65\n",
      "Iter-2524, training loss1.73\n",
      "Iter-2525, training loss1.85\n",
      "Iter-2526, training loss1.79\n",
      "Iter-2527, training loss1.83\n",
      "Iter-2528, training loss1.65\n",
      "Iter-2529, training loss1.73\n",
      "Iter-2530, training loss1.90\n",
      "Iter-2531, training loss1.70\n",
      "Iter-2532, training loss1.87\n",
      "Iter-2533, training loss1.72\n",
      "Iter-2534, training loss1.67\n",
      "Iter-2535, training loss1.85\n",
      "Iter-2536, training loss1.79\n",
      "Iter-2537, training loss1.71\n",
      "Iter-2538, training loss1.89\n",
      "Iter-2539, training loss1.75\n",
      "Iter-2540, training loss1.79\n",
      "Iter-2541, training loss1.69\n",
      "Iter-2542, training loss1.54\n",
      "Iter-2543, training loss1.78\n",
      "Iter-2544, training loss1.81\n",
      "Iter-2545, training loss1.86\n",
      "Iter-2546, training loss1.66\n",
      "Iter-2547, training loss1.69\n",
      "Iter-2548, training loss1.67\n",
      "Iter-2549, training loss1.70\n",
      "Iter-2550, training loss1.79\n",
      "Iter-2551, training loss1.78\n",
      "Iter-2552, training loss1.89\n",
      "Iter-2553, training loss1.93\n",
      "Iter-2554, training loss1.77\n",
      "Iter-2555, training loss1.64\n",
      "Iter-2556, training loss1.72\n",
      "Iter-2557, training loss1.67\n",
      "Iter-2558, training loss1.76\n",
      "Iter-2559, training loss1.92\n",
      "Iter-2560, training loss1.73\n",
      "Iter-2561, training loss1.70\n",
      "Iter-2562, training loss1.78\n",
      "Iter-2563, training loss1.82\n",
      "Iter-2564, training loss1.77\n",
      "Iter-2565, training loss1.78\n",
      "Iter-2566, training loss1.76\n",
      "Iter-2567, training loss1.65\n",
      "Iter-2568, training loss1.65\n",
      "Iter-2569, training loss1.69\n",
      "Iter-2570, training loss1.59\n",
      "Iter-2571, training loss1.68\n",
      "Iter-2572, training loss1.85\n",
      "Iter-2573, training loss1.75\n",
      "Iter-2574, training loss1.77\n",
      "Iter-2575, training loss1.74\n",
      "Iter-2576, training loss1.81\n",
      "Iter-2577, training loss1.71\n",
      "Iter-2578, training loss1.75\n",
      "Iter-2579, training loss1.72\n",
      "Iter-2580, training loss1.64\n",
      "Iter-2581, training loss1.76\n",
      "Iter-2582, training loss1.74\n",
      "Iter-2583, training loss1.69\n",
      "Iter-2584, training loss1.81\n",
      "Iter-2585, training loss1.86\n",
      "Iter-2586, training loss1.71\n",
      "Iter-2587, training loss1.94\n",
      "Iter-2588, training loss1.69\n",
      "Iter-2589, training loss1.70\n",
      "Iter-2590, training loss1.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2591, training loss1.63\n",
      "Iter-2592, training loss1.72\n",
      "Iter-2593, training loss1.71\n",
      "Iter-2594, training loss1.78\n",
      "Iter-2595, training loss1.71\n",
      "Iter-2596, training loss1.63\n",
      "Iter-2597, training loss1.75\n",
      "Iter-2598, training loss1.82\n",
      "Iter-2599, training loss1.86\n",
      "Iter-2600, training loss1.69\n",
      "Iter-2601, training loss1.65\n",
      "Iter-2602, training loss1.78\n",
      "Iter-2603, training loss1.71\n",
      "Iter-2604, training loss1.68\n",
      "Iter-2605, training loss1.75\n",
      "Iter-2606, training loss1.70\n",
      "Iter-2607, training loss1.86\n",
      "Iter-2608, training loss1.69\n",
      "Iter-2609, training loss1.63\n",
      "Iter-2610, training loss1.79\n",
      "Iter-2611, training loss1.76\n",
      "Iter-2612, training loss1.79\n",
      "Iter-2613, training loss1.81\n",
      "Iter-2614, training loss1.70\n",
      "Iter-2615, training loss1.74\n",
      "Iter-2616, training loss1.75\n",
      "Iter-2617, training loss1.71\n",
      "Iter-2618, training loss1.64\n",
      "Iter-2619, training loss1.77\n",
      "Iter-2620, training loss1.65\n",
      "Iter-2621, training loss1.72\n",
      "Iter-2622, training loss1.79\n",
      "Iter-2623, training loss1.74\n",
      "Iter-2624, training loss1.73\n",
      "Iter-2625, training loss1.79\n",
      "Iter-2626, training loss1.70\n",
      "Iter-2627, training loss1.57\n",
      "Iter-2628, training loss1.73\n",
      "Iter-2629, training loss1.69\n",
      "Iter-2630, training loss1.80\n",
      "Iter-2631, training loss1.65\n",
      "Iter-2632, training loss1.70\n",
      "Iter-2633, training loss1.62\n",
      "Iter-2634, training loss1.67\n",
      "Iter-2635, training loss1.77\n",
      "Iter-2636, training loss1.70\n",
      "Iter-2637, training loss1.90\n",
      "Iter-2638, training loss1.76\n",
      "Iter-2639, training loss1.74\n",
      "Iter-2640, training loss1.73\n",
      "Iter-2641, training loss1.71\n",
      "Iter-2642, training loss1.76\n",
      "Iter-2643, training loss1.91\n",
      "Iter-2644, training loss1.79\n",
      "Iter-2645, training loss1.57\n",
      "Iter-2646, training loss1.69\n",
      "Iter-2647, training loss1.60\n",
      "Iter-2648, training loss1.66\n",
      "Iter-2649, training loss1.69\n",
      "Iter-2650, training loss1.66\n",
      "Iter-2651, training loss1.81\n",
      "Iter-2652, training loss1.71\n",
      "Iter-2653, training loss1.67\n",
      "Iter-2654, training loss1.82\n",
      "Iter-2655, training loss1.71\n",
      "Iter-2656, training loss1.81\n",
      "Iter-2657, training loss1.66\n",
      "Iter-2658, training loss1.69\n",
      "Iter-2659, training loss1.74\n",
      "Iter-2660, training loss1.75\n",
      "Iter-2661, training loss1.72\n",
      "Iter-2662, training loss1.73\n",
      "Iter-2663, training loss1.75\n",
      "Iter-2664, training loss1.81\n",
      "Iter-2665, training loss1.78\n",
      "Iter-2666, training loss1.65\n",
      "Iter-2667, training loss1.76\n",
      "Iter-2668, training loss1.68\n",
      "Iter-2669, training loss1.65\n",
      "Iter-2670, training loss1.55\n",
      "Iter-2671, training loss1.51\n",
      "Iter-2672, training loss1.69\n",
      "Iter-2673, training loss1.59\n",
      "Iter-2674, training loss1.64\n",
      "Iter-2675, training loss1.68\n",
      "Iter-2676, training loss1.75\n",
      "Iter-2677, training loss1.69\n",
      "Iter-2678, training loss1.84\n",
      "Iter-2679, training loss1.92\n",
      "Iter-2680, training loss1.85\n",
      "Iter-2681, training loss1.76\n",
      "Iter-2682, training loss1.78\n",
      "Iter-2683, training loss1.84\n",
      "Iter-2684, training loss1.68\n",
      "Iter-2685, training loss1.74\n",
      "Iter-2686, training loss1.79\n",
      "Iter-2687, training loss1.71\n",
      "Iter-2688, training loss1.80\n",
      "Iter-2689, training loss1.76\n",
      "Iter-2690, training loss1.82\n",
      "Iter-2691, training loss1.74\n",
      "Iter-2692, training loss1.78\n",
      "Iter-2693, training loss1.71\n",
      "Iter-2694, training loss1.67\n",
      "Iter-2695, training loss1.72\n",
      "Iter-2696, training loss1.74\n",
      "Iter-2697, training loss1.69\n",
      "Iter-2698, training loss1.63\n",
      "Iter-2699, training loss1.74\n",
      "Iter-2700, training loss1.76\n",
      "Iter-2701, training loss1.78\n",
      "Iter-2702, training loss1.63\n",
      "Iter-2703, training loss1.70\n",
      "Iter-2704, training loss1.81\n",
      "Iter-2705, training loss1.73\n",
      "Iter-2706, training loss1.80\n",
      "Iter-2707, training loss1.76\n",
      "Iter-2708, training loss1.68\n",
      "Iter-2709, training loss1.81\n",
      "Iter-2710, training loss1.70\n",
      "Iter-2711, training loss1.84\n",
      "Iter-2712, training loss1.63\n",
      "Iter-2713, training loss1.72\n",
      "Iter-2714, training loss1.83\n",
      "Iter-2715, training loss1.72\n",
      "Iter-2716, training loss1.75\n",
      "Iter-2717, training loss1.78\n",
      "Iter-2718, training loss1.66\n",
      "Iter-2719, training loss1.83\n",
      "Iter-2720, training loss1.79\n",
      "Iter-2721, training loss1.79\n",
      "Iter-2722, training loss1.77\n",
      "Iter-2723, training loss1.73\n",
      "Iter-2724, training loss1.58\n",
      "Iter-2725, training loss1.69\n",
      "Iter-2726, training loss1.70\n",
      "Iter-2727, training loss1.85\n",
      "Iter-2728, training loss1.77\n",
      "Iter-2729, training loss1.85\n",
      "Iter-2730, training loss1.59\n",
      "Iter-2731, training loss1.79\n",
      "Iter-2732, training loss1.78\n",
      "Iter-2733, training loss1.72\n",
      "Iter-2734, training loss1.66\n",
      "Iter-2735, training loss1.71\n",
      "Iter-2736, training loss1.83\n",
      "Iter-2737, training loss1.79\n",
      "Iter-2738, training loss1.82\n",
      "Iter-2739, training loss1.86\n",
      "Iter-2740, training loss1.80\n",
      "Iter-2741, training loss1.61\n",
      "Iter-2742, training loss1.79\n",
      "Iter-2743, training loss1.75\n",
      "Iter-2744, training loss1.60\n",
      "Iter-2745, training loss1.65\n",
      "Iter-2746, training loss1.73\n",
      "Iter-2747, training loss1.64\n",
      "Iter-2748, training loss1.72\n",
      "Iter-2749, training loss1.70\n",
      "Iter-2750, training loss1.62\n",
      "Iter-2751, training loss1.61\n",
      "Iter-2752, training loss1.86\n",
      "Iter-2753, training loss1.66\n",
      "Iter-2754, training loss1.76\n",
      "Iter-2755, training loss1.73\n",
      "Iter-2756, training loss1.56\n",
      "Iter-2757, training loss1.75\n",
      "Iter-2758, training loss1.69\n",
      "Iter-2759, training loss1.78\n",
      "Iter-2760, training loss1.67\n",
      "Iter-2761, training loss1.74\n",
      "Iter-2762, training loss1.65\n",
      "Iter-2763, training loss1.58\n",
      "Iter-2764, training loss1.60\n",
      "Iter-2765, training loss1.65\n",
      "Iter-2766, training loss1.80\n",
      "Iter-2767, training loss1.78\n",
      "Iter-2768, training loss1.68\n",
      "Iter-2769, training loss1.76\n",
      "Iter-2770, training loss1.78\n",
      "Iter-2771, training loss1.68\n",
      "Iter-2772, training loss1.72\n",
      "Iter-2773, training loss1.68\n",
      "Iter-2774, training loss1.70\n",
      "Iter-2775, training loss1.80\n",
      "Iter-2776, training loss1.55\n",
      "Iter-2777, training loss1.56\n",
      "Iter-2778, training loss1.61\n",
      "Iter-2779, training loss1.65\n",
      "Iter-2780, training loss1.74\n",
      "Iter-2781, training loss1.62\n",
      "Iter-2782, training loss1.80\n",
      "Iter-2783, training loss1.76\n",
      "Iter-2784, training loss1.67\n",
      "Iter-2785, training loss1.89\n",
      "Iter-2786, training loss1.75\n",
      "Iter-2787, training loss1.68\n",
      "Iter-2788, training loss1.68\n",
      "Iter-2789, training loss1.70\n",
      "Iter-2790, training loss1.70\n",
      "Iter-2791, training loss1.79\n",
      "Iter-2792, training loss1.64\n",
      "Iter-2793, training loss1.79\n",
      "Iter-2794, training loss1.74\n",
      "Iter-2795, training loss1.69\n",
      "Iter-2796, training loss1.63\n",
      "Iter-2797, training loss1.56\n",
      "Iter-2798, training loss1.75\n",
      "Iter-2799, training loss1.79\n",
      "Iter-2800, training loss1.70\n",
      "Iter-2801, training loss1.82\n",
      "Iter-2802, training loss1.64\n",
      "Iter-2803, training loss1.81\n",
      "Iter-2804, training loss1.73\n",
      "Iter-2805, training loss1.65\n",
      "Iter-2806, training loss1.66\n",
      "Iter-2807, training loss1.67\n",
      "Iter-2808, training loss1.86\n",
      "Iter-2809, training loss1.82\n",
      "Iter-2810, training loss1.83\n",
      "Iter-2811, training loss1.69\n",
      "Iter-2812, training loss1.64\n",
      "Iter-2813, training loss1.83\n",
      "Iter-2814, training loss1.88\n",
      "Iter-2815, training loss1.57\n",
      "Iter-2816, training loss1.71\n",
      "Iter-2817, training loss1.69\n",
      "Iter-2818, training loss1.70\n",
      "Iter-2819, training loss1.80\n",
      "Iter-2820, training loss1.75\n",
      "Iter-2821, training loss1.79\n",
      "Iter-2822, training loss1.56\n",
      "Iter-2823, training loss1.76\n",
      "Iter-2824, training loss1.89\n",
      "Iter-2825, training loss1.81\n",
      "Iter-2826, training loss1.78\n",
      "Iter-2827, training loss1.79\n",
      "Iter-2828, training loss1.76\n",
      "Iter-2829, training loss1.65\n",
      "Iter-2830, training loss1.69\n",
      "Iter-2831, training loss1.79\n",
      "Iter-2832, training loss1.68\n",
      "Iter-2833, training loss1.61\n",
      "Iter-2834, training loss1.77\n",
      "Iter-2835, training loss1.68\n",
      "Iter-2836, training loss1.71\n",
      "Iter-2837, training loss1.72\n",
      "Iter-2838, training loss1.61\n",
      "Iter-2839, training loss1.78\n",
      "Iter-2840, training loss1.86\n",
      "Iter-2841, training loss1.57\n",
      "Iter-2842, training loss1.88\n",
      "Iter-2843, training loss1.68\n",
      "Iter-2844, training loss1.64\n",
      "Iter-2845, training loss1.95\n",
      "Iter-2846, training loss1.58\n",
      "Iter-2847, training loss1.68\n",
      "Iter-2848, training loss1.72\n",
      "Iter-2849, training loss1.86\n",
      "Iter-2850, training loss1.66\n",
      "Iter-2851, training loss1.75\n",
      "Iter-2852, training loss1.60\n",
      "Iter-2853, training loss1.82\n",
      "Iter-2854, training loss1.68\n",
      "Iter-2855, training loss1.52\n",
      "Iter-2856, training loss1.85\n",
      "Iter-2857, training loss1.81\n",
      "Iter-2858, training loss1.72\n",
      "Iter-2859, training loss1.58\n",
      "Iter-2860, training loss1.80\n",
      "Iter-2861, training loss1.68\n",
      "Iter-2862, training loss1.80\n",
      "Iter-2863, training loss1.77\n",
      "Iter-2864, training loss1.63\n",
      "Iter-2865, training loss1.74\n",
      "Iter-2866, training loss1.62\n",
      "Iter-2867, training loss1.70\n",
      "Iter-2868, training loss1.47\n",
      "Iter-2869, training loss1.75\n",
      "Iter-2870, training loss1.75\n",
      "Iter-2871, training loss1.65\n",
      "Iter-2872, training loss1.68\n",
      "Iter-2873, training loss1.65\n",
      "Iter-2874, training loss1.74\n",
      "Iter-2875, training loss1.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2876, training loss1.85\n",
      "Iter-2877, training loss1.71\n",
      "Iter-2878, training loss1.95\n",
      "Iter-2879, training loss1.65\n",
      "Iter-2880, training loss1.86\n",
      "Iter-2881, training loss1.62\n",
      "Iter-2882, training loss1.65\n",
      "Iter-2883, training loss1.73\n",
      "Iter-2884, training loss1.86\n",
      "Iter-2885, training loss1.66\n",
      "Iter-2886, training loss1.91\n",
      "Iter-2887, training loss1.86\n",
      "Iter-2888, training loss1.65\n",
      "Iter-2889, training loss1.75\n",
      "Iter-2890, training loss1.67\n",
      "Iter-2891, training loss1.73\n",
      "Iter-2892, training loss1.80\n",
      "Iter-2893, training loss1.66\n",
      "Iter-2894, training loss1.58\n",
      "Iter-2895, training loss1.64\n",
      "Iter-2896, training loss1.85\n",
      "Iter-2897, training loss1.81\n",
      "Iter-2898, training loss1.59\n",
      "Iter-2899, training loss1.74\n",
      "Iter-2900, training loss1.83\n",
      "Iter-2901, training loss1.74\n",
      "Iter-2902, training loss1.81\n",
      "Iter-2903, training loss1.72\n",
      "Iter-2904, training loss1.63\n",
      "Iter-2905, training loss1.62\n",
      "Iter-2906, training loss1.71\n",
      "Iter-2907, training loss1.76\n",
      "Iter-2908, training loss1.61\n",
      "Iter-2909, training loss1.82\n",
      "Iter-2910, training loss1.92\n",
      "Iter-2911, training loss1.73\n",
      "Iter-2912, training loss1.69\n",
      "Iter-2913, training loss1.61\n",
      "Iter-2914, training loss1.77\n",
      "Iter-2915, training loss1.76\n",
      "Iter-2916, training loss1.60\n",
      "Iter-2917, training loss1.69\n",
      "Iter-2918, training loss1.52\n",
      "Iter-2919, training loss1.80\n",
      "Iter-2920, training loss1.70\n",
      "Iter-2921, training loss1.89\n",
      "Iter-2922, training loss1.71\n",
      "Iter-2923, training loss1.67\n",
      "Iter-2924, training loss1.89\n",
      "Iter-2925, training loss1.75\n",
      "Iter-2926, training loss1.71\n",
      "Iter-2927, training loss1.63\n",
      "Iter-2928, training loss1.71\n",
      "Iter-2929, training loss1.71\n",
      "Iter-2930, training loss1.74\n",
      "Iter-2931, training loss1.77\n",
      "Iter-2932, training loss1.70\n",
      "Iter-2933, training loss1.67\n",
      "Iter-2934, training loss1.68\n",
      "Iter-2935, training loss1.66\n",
      "Iter-2936, training loss1.68\n",
      "Iter-2937, training loss1.72\n",
      "Iter-2938, training loss1.74\n",
      "Iter-2939, training loss1.85\n",
      "Iter-2940, training loss1.87\n",
      "Iter-2941, training loss1.71\n",
      "Iter-2942, training loss1.74\n",
      "Iter-2943, training loss1.71\n",
      "Iter-2944, training loss1.60\n",
      "Iter-2945, training loss1.75\n",
      "Iter-2946, training loss1.78\n",
      "Iter-2947, training loss1.69\n",
      "Iter-2948, training loss1.60\n",
      "Iter-2949, training loss1.76\n",
      "Iter-2950, training loss1.79\n",
      "Iter-2951, training loss1.67\n",
      "Iter-2952, training loss1.69\n",
      "Iter-2953, training loss1.78\n",
      "Iter-2954, training loss1.89\n",
      "Iter-2955, training loss1.65\n",
      "Iter-2956, training loss1.61\n",
      "Iter-2957, training loss1.72\n",
      "Iter-2958, training loss1.73\n",
      "Iter-2959, training loss1.69\n",
      "Iter-2960, training loss1.67\n",
      "Iter-2961, training loss1.75\n",
      "Iter-2962, training loss1.59\n",
      "Iter-2963, training loss1.72\n",
      "Iter-2964, training loss1.74\n",
      "Iter-2965, training loss1.72\n",
      "Iter-2966, training loss1.80\n",
      "Iter-2967, training loss1.67\n",
      "Iter-2968, training loss1.78\n",
      "Iter-2969, training loss1.80\n",
      "Iter-2970, training loss1.71\n",
      "Iter-2971, training loss1.64\n",
      "Iter-2972, training loss1.76\n",
      "Iter-2973, training loss1.75\n",
      "Iter-2974, training loss1.78\n",
      "Iter-2975, training loss1.87\n",
      "Iter-2976, training loss1.69\n",
      "Iter-2977, training loss1.77\n",
      "Iter-2978, training loss1.75\n",
      "Iter-2979, training loss1.60\n",
      "Iter-2980, training loss1.69\n",
      "Iter-2981, training loss1.85\n",
      "Iter-2982, training loss1.79\n",
      "Iter-2983, training loss1.72\n",
      "Iter-2984, training loss1.65\n",
      "Iter-2985, training loss1.74\n",
      "Iter-2986, training loss1.59\n",
      "Iter-2987, training loss1.85\n",
      "Iter-2988, training loss1.79\n",
      "Iter-2989, training loss1.75\n",
      "Iter-2990, training loss1.71\n",
      "Iter-2991, training loss1.77\n",
      "Iter-2992, training loss1.84\n",
      "Iter-2993, training loss1.65\n",
      "Iter-2994, training loss1.67\n",
      "Iter-2995, training loss1.69\n",
      "Iter-2996, training loss1.77\n",
      "Iter-2997, training loss1.88\n",
      "Iter-2998, training loss1.86\n",
      "Iter-2999, training loss1.75\n",
      "Iter-3000, training loss1.60\n",
      "Epoch-7\n",
      "-----------------\n",
      "Iter-3001, training loss1.65\n",
      "Iter-3002, training loss1.75\n",
      "Iter-3003, training loss1.75\n",
      "Iter-3004, training loss1.65\n",
      "Iter-3005, training loss1.74\n",
      "Iter-3006, training loss1.78\n",
      "Iter-3007, training loss1.84\n",
      "Iter-3008, training loss1.73\n",
      "Iter-3009, training loss1.71\n",
      "Iter-3010, training loss1.93\n",
      "Iter-3011, training loss1.85\n",
      "Iter-3012, training loss1.69\n",
      "Iter-3013, training loss1.65\n",
      "Iter-3014, training loss1.71\n",
      "Iter-3015, training loss1.70\n",
      "Iter-3016, training loss1.80\n",
      "Iter-3017, training loss1.64\n",
      "Iter-3018, training loss1.74\n",
      "Iter-3019, training loss1.53\n",
      "Iter-3020, training loss1.50\n",
      "Iter-3021, training loss1.66\n",
      "Iter-3022, training loss1.77\n",
      "Iter-3023, training loss1.79\n",
      "Iter-3024, training loss1.69\n",
      "Iter-3025, training loss1.53\n",
      "Iter-3026, training loss1.56\n",
      "Iter-3027, training loss1.72\n",
      "Iter-3028, training loss1.68\n",
      "Iter-3029, training loss1.61\n",
      "Iter-3030, training loss1.79\n",
      "Iter-3031, training loss1.83\n",
      "Iter-3032, training loss1.73\n",
      "Iter-3033, training loss1.78\n",
      "Iter-3034, training loss1.75\n",
      "Iter-3035, training loss1.59\n",
      "Iter-3036, training loss1.68\n",
      "Iter-3037, training loss1.68\n",
      "Iter-3038, training loss1.94\n",
      "Iter-3039, training loss1.81\n",
      "Iter-3040, training loss1.79\n",
      "Iter-3041, training loss1.78\n",
      "Iter-3042, training loss1.75\n",
      "Iter-3043, training loss1.59\n",
      "Iter-3044, training loss1.67\n",
      "Iter-3045, training loss1.72\n",
      "Iter-3046, training loss1.88\n",
      "Iter-3047, training loss1.62\n",
      "Iter-3048, training loss1.86\n",
      "Iter-3049, training loss1.68\n",
      "Iter-3050, training loss1.58\n",
      "Iter-3051, training loss1.82\n",
      "Iter-3052, training loss1.61\n",
      "Iter-3053, training loss1.73\n",
      "Iter-3054, training loss1.82\n",
      "Iter-3055, training loss1.74\n",
      "Iter-3056, training loss1.76\n",
      "Iter-3057, training loss1.62\n",
      "Iter-3058, training loss1.77\n",
      "Iter-3059, training loss1.60\n",
      "Iter-3060, training loss1.60\n",
      "Iter-3061, training loss1.65\n",
      "Iter-3062, training loss1.62\n",
      "Iter-3063, training loss1.68\n",
      "Iter-3064, training loss1.65\n",
      "Iter-3065, training loss1.58\n",
      "Iter-3066, training loss1.58\n",
      "Iter-3067, training loss1.83\n",
      "Iter-3068, training loss1.85\n",
      "Iter-3069, training loss1.72\n",
      "Iter-3070, training loss1.66\n",
      "Iter-3071, training loss1.75\n",
      "Iter-3072, training loss1.75\n",
      "Iter-3073, training loss1.66\n",
      "Iter-3074, training loss1.59\n",
      "Iter-3075, training loss1.80\n",
      "Iter-3076, training loss1.64\n",
      "Iter-3077, training loss1.71\n",
      "Iter-3078, training loss1.77\n",
      "Iter-3079, training loss1.61\n",
      "Iter-3080, training loss1.58\n",
      "Iter-3081, training loss1.57\n",
      "Iter-3082, training loss1.65\n",
      "Iter-3083, training loss1.88\n",
      "Iter-3084, training loss1.67\n",
      "Iter-3085, training loss1.79\n",
      "Iter-3086, training loss1.67\n",
      "Iter-3087, training loss1.74\n",
      "Iter-3088, training loss1.62\n",
      "Iter-3089, training loss1.80\n",
      "Iter-3090, training loss1.64\n",
      "Iter-3091, training loss1.73\n",
      "Iter-3092, training loss1.76\n",
      "Iter-3093, training loss1.63\n",
      "Iter-3094, training loss1.57\n",
      "Iter-3095, training loss1.94\n",
      "Iter-3096, training loss1.67\n",
      "Iter-3097, training loss1.72\n",
      "Iter-3098, training loss1.86\n",
      "Iter-3099, training loss1.73\n",
      "Iter-3100, training loss1.64\n",
      "Iter-3101, training loss1.57\n",
      "Iter-3102, training loss1.64\n",
      "Iter-3103, training loss1.68\n",
      "Iter-3104, training loss1.82\n",
      "Iter-3105, training loss1.72\n",
      "Iter-3106, training loss1.78\n",
      "Iter-3107, training loss1.76\n",
      "Iter-3108, training loss1.84\n",
      "Iter-3109, training loss1.68\n",
      "Iter-3110, training loss1.72\n",
      "Iter-3111, training loss1.72\n",
      "Iter-3112, training loss1.69\n",
      "Iter-3113, training loss1.54\n",
      "Iter-3114, training loss1.57\n",
      "Iter-3115, training loss1.69\n",
      "Iter-3116, training loss1.83\n",
      "Iter-3117, training loss1.73\n",
      "Iter-3118, training loss1.76\n",
      "Iter-3119, training loss1.65\n",
      "Iter-3120, training loss1.57\n",
      "Iter-3121, training loss1.76\n",
      "Iter-3122, training loss1.76\n",
      "Iter-3123, training loss1.78\n",
      "Iter-3124, training loss1.85\n",
      "Iter-3125, training loss1.68\n",
      "Iter-3126, training loss1.78\n",
      "Iter-3127, training loss1.71\n",
      "Iter-3128, training loss1.61\n",
      "Iter-3129, training loss1.71\n",
      "Iter-3130, training loss1.65\n",
      "Iter-3131, training loss1.71\n",
      "Iter-3132, training loss1.73\n",
      "Iter-3133, training loss1.65\n",
      "Iter-3134, training loss1.67\n",
      "Iter-3135, training loss1.63\n",
      "Iter-3136, training loss1.76\n",
      "Iter-3137, training loss1.68\n",
      "Iter-3138, training loss1.71\n",
      "Iter-3139, training loss1.82\n",
      "Iter-3140, training loss1.61\n",
      "Iter-3141, training loss1.73\n",
      "Iter-3142, training loss1.74\n",
      "Iter-3143, training loss1.79\n",
      "Iter-3144, training loss1.71\n",
      "Iter-3145, training loss1.64\n",
      "Iter-3146, training loss1.60\n",
      "Iter-3147, training loss1.58\n",
      "Iter-3148, training loss1.57\n",
      "Iter-3149, training loss1.64\n",
      "Iter-3150, training loss1.70\n",
      "Iter-3151, training loss1.66\n",
      "Iter-3152, training loss1.74\n",
      "Iter-3153, training loss1.65\n",
      "Iter-3154, training loss1.64\n",
      "Iter-3155, training loss1.56\n",
      "Iter-3156, training loss1.73\n",
      "Iter-3157, training loss1.75\n",
      "Iter-3158, training loss1.71\n",
      "Iter-3159, training loss1.69\n",
      "Iter-3160, training loss1.81\n",
      "Iter-3161, training loss1.90\n",
      "Iter-3162, training loss1.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3163, training loss1.90\n",
      "Iter-3164, training loss1.74\n",
      "Iter-3165, training loss1.69\n",
      "Iter-3166, training loss1.78\n",
      "Iter-3167, training loss1.68\n",
      "Iter-3168, training loss1.81\n",
      "Iter-3169, training loss1.70\n",
      "Iter-3170, training loss1.82\n",
      "Iter-3171, training loss1.66\n",
      "Iter-3172, training loss1.70\n",
      "Iter-3173, training loss1.65\n",
      "Iter-3174, training loss1.63\n",
      "Iter-3175, training loss1.78\n",
      "Iter-3176, training loss1.69\n",
      "Iter-3177, training loss1.82\n",
      "Iter-3178, training loss1.74\n",
      "Iter-3179, training loss1.66\n",
      "Iter-3180, training loss1.59\n",
      "Iter-3181, training loss1.76\n",
      "Iter-3182, training loss1.79\n",
      "Iter-3183, training loss1.63\n",
      "Iter-3184, training loss1.81\n",
      "Iter-3185, training loss1.66\n",
      "Iter-3186, training loss1.70\n",
      "Iter-3187, training loss1.60\n",
      "Iter-3188, training loss1.77\n",
      "Iter-3189, training loss1.66\n",
      "Iter-3190, training loss1.70\n",
      "Iter-3191, training loss1.59\n",
      "Iter-3192, training loss1.55\n",
      "Iter-3193, training loss1.57\n",
      "Iter-3194, training loss1.61\n",
      "Iter-3195, training loss1.62\n",
      "Iter-3196, training loss1.62\n",
      "Iter-3197, training loss1.63\n",
      "Iter-3198, training loss1.72\n",
      "Iter-3199, training loss1.49\n",
      "Iter-3200, training loss1.75\n",
      "Iter-3201, training loss1.71\n",
      "Iter-3202, training loss1.66\n",
      "Iter-3203, training loss1.66\n",
      "Iter-3204, training loss1.72\n",
      "Iter-3205, training loss1.76\n",
      "Iter-3206, training loss1.77\n",
      "Iter-3207, training loss1.70\n",
      "Iter-3208, training loss1.75\n",
      "Iter-3209, training loss1.55\n",
      "Iter-3210, training loss1.75\n",
      "Iter-3211, training loss1.68\n",
      "Iter-3212, training loss1.71\n",
      "Iter-3213, training loss1.67\n",
      "Iter-3214, training loss1.80\n",
      "Iter-3215, training loss1.73\n",
      "Iter-3216, training loss1.72\n",
      "Iter-3217, training loss1.55\n",
      "Iter-3218, training loss1.83\n",
      "Iter-3219, training loss1.75\n",
      "Iter-3220, training loss1.60\n",
      "Iter-3221, training loss1.60\n",
      "Iter-3222, training loss1.69\n",
      "Iter-3223, training loss1.70\n",
      "Iter-3224, training loss1.71\n",
      "Iter-3225, training loss1.78\n",
      "Iter-3226, training loss1.78\n",
      "Iter-3227, training loss1.69\n",
      "Iter-3228, training loss1.85\n",
      "Iter-3229, training loss1.58\n",
      "Iter-3230, training loss1.61\n",
      "Iter-3231, training loss1.68\n",
      "Iter-3232, training loss1.71\n",
      "Iter-3233, training loss1.70\n",
      "Iter-3234, training loss1.65\n",
      "Iter-3235, training loss1.67\n",
      "Iter-3236, training loss1.78\n",
      "Iter-3237, training loss1.61\n",
      "Iter-3238, training loss1.83\n",
      "Iter-3239, training loss1.90\n",
      "Iter-3240, training loss1.60\n",
      "Iter-3241, training loss1.86\n",
      "Iter-3242, training loss1.68\n",
      "Iter-3243, training loss1.92\n",
      "Iter-3244, training loss1.68\n",
      "Iter-3245, training loss1.68\n",
      "Iter-3246, training loss1.62\n",
      "Iter-3247, training loss1.76\n",
      "Iter-3248, training loss1.68\n",
      "Iter-3249, training loss1.60\n",
      "Iter-3250, training loss1.59\n",
      "Iter-3251, training loss1.89\n",
      "Iter-3252, training loss1.64\n",
      "Iter-3253, training loss1.65\n",
      "Iter-3254, training loss1.72\n",
      "Iter-3255, training loss1.68\n",
      "Iter-3256, training loss1.65\n",
      "Iter-3257, training loss1.70\n",
      "Iter-3258, training loss1.68\n",
      "Iter-3259, training loss1.77\n",
      "Iter-3260, training loss1.73\n",
      "Iter-3261, training loss1.62\n",
      "Iter-3262, training loss1.71\n",
      "Iter-3263, training loss1.72\n",
      "Iter-3264, training loss1.61\n",
      "Iter-3265, training loss1.66\n",
      "Iter-3266, training loss1.74\n",
      "Iter-3267, training loss1.77\n",
      "Iter-3268, training loss1.65\n",
      "Iter-3269, training loss1.78\n",
      "Iter-3270, training loss1.65\n",
      "Iter-3271, training loss1.71\n",
      "Iter-3272, training loss1.69\n",
      "Iter-3273, training loss1.70\n",
      "Iter-3274, training loss1.78\n",
      "Iter-3275, training loss1.61\n",
      "Iter-3276, training loss1.78\n",
      "Iter-3277, training loss1.71\n",
      "Iter-3278, training loss1.81\n",
      "Iter-3279, training loss1.78\n",
      "Iter-3280, training loss1.64\n",
      "Iter-3281, training loss1.52\n",
      "Iter-3282, training loss1.81\n",
      "Iter-3283, training loss1.64\n",
      "Iter-3284, training loss1.97\n",
      "Iter-3285, training loss1.80\n",
      "Iter-3286, training loss1.88\n",
      "Iter-3287, training loss1.77\n",
      "Iter-3288, training loss1.72\n",
      "Iter-3289, training loss1.72\n",
      "Iter-3290, training loss1.73\n",
      "Iter-3291, training loss1.66\n",
      "Iter-3292, training loss1.75\n",
      "Iter-3293, training loss1.70\n",
      "Iter-3294, training loss1.63\n",
      "Iter-3295, training loss1.81\n",
      "Iter-3296, training loss1.72\n",
      "Iter-3297, training loss1.65\n",
      "Iter-3298, training loss1.61\n",
      "Iter-3299, training loss1.68\n",
      "Iter-3300, training loss1.68\n",
      "Iter-3301, training loss1.70\n",
      "Iter-3302, training loss1.67\n",
      "Iter-3303, training loss1.66\n",
      "Iter-3304, training loss1.72\n",
      "Iter-3305, training loss1.49\n",
      "Iter-3306, training loss1.81\n",
      "Iter-3307, training loss1.69\n",
      "Iter-3308, training loss1.81\n",
      "Iter-3309, training loss1.67\n",
      "Iter-3310, training loss1.57\n",
      "Iter-3311, training loss1.80\n",
      "Iter-3312, training loss1.65\n",
      "Iter-3313, training loss1.72\n",
      "Iter-3314, training loss1.63\n",
      "Iter-3315, training loss1.59\n",
      "Iter-3316, training loss1.88\n",
      "Iter-3317, training loss1.73\n",
      "Iter-3318, training loss1.58\n",
      "Iter-3319, training loss1.83\n",
      "Iter-3320, training loss1.69\n",
      "Iter-3321, training loss1.69\n",
      "Iter-3322, training loss1.90\n",
      "Iter-3323, training loss1.63\n",
      "Iter-3324, training loss1.72\n",
      "Iter-3325, training loss1.73\n",
      "Iter-3326, training loss1.87\n",
      "Iter-3327, training loss1.72\n",
      "Iter-3328, training loss1.63\n",
      "Iter-3329, training loss1.82\n",
      "Iter-3330, training loss1.77\n",
      "Iter-3331, training loss1.64\n",
      "Iter-3332, training loss1.67\n",
      "Iter-3333, training loss1.73\n",
      "Iter-3334, training loss1.83\n",
      "Iter-3335, training loss1.66\n",
      "Iter-3336, training loss1.79\n",
      "Iter-3337, training loss1.74\n",
      "Iter-3338, training loss1.66\n",
      "Iter-3339, training loss1.85\n",
      "Iter-3340, training loss1.66\n",
      "Iter-3341, training loss1.66\n",
      "Iter-3342, training loss1.76\n",
      "Iter-3343, training loss1.78\n",
      "Iter-3344, training loss1.67\n",
      "Iter-3345, training loss1.68\n",
      "Iter-3346, training loss1.73\n",
      "Iter-3347, training loss1.74\n",
      "Iter-3348, training loss1.68\n",
      "Iter-3349, training loss1.80\n",
      "Iter-3350, training loss1.61\n",
      "Iter-3351, training loss1.66\n",
      "Iter-3352, training loss1.60\n",
      "Iter-3353, training loss1.71\n",
      "Iter-3354, training loss1.61\n",
      "Iter-3355, training loss1.63\n",
      "Iter-3356, training loss1.82\n",
      "Iter-3357, training loss1.68\n",
      "Iter-3358, training loss1.59\n",
      "Iter-3359, training loss1.59\n",
      "Iter-3360, training loss1.52\n",
      "Iter-3361, training loss1.80\n",
      "Iter-3362, training loss1.77\n",
      "Iter-3363, training loss1.67\n",
      "Iter-3364, training loss1.72\n",
      "Iter-3365, training loss1.80\n",
      "Iter-3366, training loss1.66\n",
      "Iter-3367, training loss1.63\n",
      "Iter-3368, training loss1.64\n",
      "Iter-3369, training loss1.77\n",
      "Iter-3370, training loss1.73\n",
      "Iter-3371, training loss1.51\n",
      "Iter-3372, training loss1.84\n",
      "Iter-3373, training loss1.80\n",
      "Iter-3374, training loss1.83\n",
      "Iter-3375, training loss1.77\n",
      "Iter-3376, training loss1.45\n",
      "Iter-3377, training loss1.64\n",
      "Iter-3378, training loss1.89\n",
      "Iter-3379, training loss1.81\n",
      "Iter-3380, training loss1.76\n",
      "Iter-3381, training loss1.69\n",
      "Iter-3382, training loss1.62\n",
      "Iter-3383, training loss1.70\n",
      "Iter-3384, training loss1.69\n",
      "Iter-3385, training loss1.62\n",
      "Iter-3386, training loss1.77\n",
      "Iter-3387, training loss1.59\n",
      "Iter-3388, training loss1.93\n",
      "Iter-3389, training loss1.76\n",
      "Iter-3390, training loss1.67\n",
      "Iter-3391, training loss1.61\n",
      "Iter-3392, training loss1.55\n",
      "Iter-3393, training loss1.44\n",
      "Iter-3394, training loss1.72\n",
      "Iter-3395, training loss1.80\n",
      "Iter-3396, training loss1.64\n",
      "Iter-3397, training loss1.79\n",
      "Iter-3398, training loss1.60\n",
      "Iter-3399, training loss1.60\n",
      "Iter-3400, training loss1.68\n",
      "Iter-3401, training loss1.76\n",
      "Iter-3402, training loss1.75\n",
      "Iter-3403, training loss1.71\n",
      "Iter-3404, training loss1.64\n",
      "Iter-3405, training loss1.70\n",
      "Iter-3406, training loss1.51\n",
      "Iter-3407, training loss1.66\n",
      "Iter-3408, training loss1.76\n",
      "Iter-3409, training loss1.81\n",
      "Iter-3410, training loss1.54\n",
      "Iter-3411, training loss1.63\n",
      "Iter-3412, training loss1.61\n",
      "Iter-3413, training loss1.60\n",
      "Iter-3414, training loss1.70\n",
      "Iter-3415, training loss1.62\n",
      "Iter-3416, training loss1.67\n",
      "Iter-3417, training loss1.58\n",
      "Iter-3418, training loss1.68\n",
      "Iter-3419, training loss1.77\n",
      "Iter-3420, training loss1.70\n",
      "Iter-3421, training loss1.71\n",
      "Iter-3422, training loss1.73\n",
      "Iter-3423, training loss1.70\n",
      "Iter-3424, training loss1.67\n",
      "Iter-3425, training loss1.66\n",
      "Iter-3426, training loss1.79\n",
      "Iter-3427, training loss1.56\n",
      "Iter-3428, training loss1.56\n",
      "Iter-3429, training loss1.59\n",
      "Iter-3430, training loss1.67\n",
      "Iter-3431, training loss1.76\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = LogisticRegression(n_in, n_hidden, n_out)\n",
    "modeleval = ModelEvaluator(model, epochs, lr, use_gpu=True)\n",
    "modeleval.train(trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeleval.plot_loss()\n",
    "accuracy_test = modeleval.test(testloader)\n",
    "print('Accuracy of model on test set {0:.2f}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrossValidation:\n",
    "    def __init__(self, k, batch_size, trainset, use_gpu):\n",
    "        '''\n",
    "        k: number of folds\n",
    "        batch_size: batch size for training\n",
    "        trainset: training data as pytorch iterator\n",
    "        use_gpu: boolean variable to use gpus\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.nm_samples = len(trainset)\n",
    "        self.indices = list(range(self.nm_samples))\n",
    "        self.trainset = trainset\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "    def kfold(self):\n",
    "        '''\n",
    "        k-fold split\n",
    "        '''\n",
    "        for i in range(self.k):\n",
    "            train_idx = [idx for j,idx in enumerate(self.indices) if j%self.k != i]\n",
    "            valid_idx = [idx for j,idx in enumerate(self.indices) if j%self.k == i]            \n",
    "            yield train_idx, valid_idx\n",
    "    \n",
    "    def trainloader_sampling(self):\n",
    "        '''\n",
    "        k-fold samples\n",
    "        '''\n",
    "        for train_idx, valid_idx in self.kfold():\n",
    "            train_sampler = SubsetRandomSampler(train_idx)\n",
    "            valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "            yield train_sampler, valid_sampler\n",
    "\n",
    "    def gridsearchCV(self, parameters):\n",
    "        '''\n",
    "        find best parameters by doing grid search with k-fold cross validation\n",
    "        '''\n",
    "        accuracy_mat = np.zeros((len(parameters['lr']), len(parameters['n_hidden'])))\n",
    "        for ii,lr in enumerate(parameters['lr']):\n",
    "            for jj,n_hidden in enumerate(parameters['n_hidden']):\n",
    "                fold_accuracy = []\n",
    "                i = 0\n",
    "                for train_sampler, valid_sampler in self.trainloader_sampling():\n",
    "                    trainloader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size, sampler=train_sampler, num_workers=2)\n",
    "                    validloader = torch.utils.data.DataLoader(self.trainset, batch_size=self.batch_size, sampler=valid_sampler, num_workers=2)    \n",
    "                    \n",
    "                    model = LogisticRegression(n_in, n_hidden, n_out)\n",
    "                    modeleval = ModelEvaluator(model, epochs, lr, use_gpu=self.use_gpu)\n",
    "                    modeleval.train(trainloader, validloader, validation=True)\n",
    "                    #modeleval.plot_loss()\n",
    "                    accuracy_valid = modeleval.test(validloader)\n",
    "                    print('Accuracy of model on validation set {0:.2f}'.format(accuracy_valid))\n",
    "                    fold_accuracy.append(accuracy_valid)\n",
    "                    i += 1\n",
    "                mean_acc = np.mean(fold_accuracy)\n",
    "                if mean_acc > np.max(accuracy_mat):\n",
    "                    best_model, best_lr, best_n_hidden = copy.deepcopy(model), lr, n_hidden\n",
    "                    # bestmodeleval = copy.deepcopy(modeleval)\n",
    "                accuracy_mat[ii, jj] = np.mean(fold_accuracy)\n",
    "        return accuracy_mat, best_model, best_lr, best_n_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run k-fold and evaluate with best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Parameters\n",
    "parameters = {'lr':[0.00001, 0.0001, 0.001, 0.01], 'n_hidden': [512, 256, 128]}\n",
    "# k fold cross validation\n",
    "k = 3\n",
    "cv = CrossValidation(k=k, batch_size=batch_size, trainset=trainset, use_gpu=True)\n",
    "accuracy_mat, best_model, best_lr, best_n_hidden = cv.gridsearchCV(parameters)\n",
    "bestmodeleval = ModelEvaluator(best_model, epochs, best_lr, use_gpu=self.use_gpu)\n",
    "# Visualization accuracy vs parameters\n",
    "fig, ax = plt.subplots()\n",
    "lr_ = [str(lr) for lr in parameters['lr']]\n",
    "hidden_ = [str(n_hidden) for n_hidden in parameters['n_hidden']]\n",
    "im, cbar = heatmap(accuracy_mat, lr_, hidden_, ax=ax,\n",
    "                   cmap='YlGn', cbarlabel='lr vs hidden_')\n",
    "texts = annotate_heatmap(im, valfmt='{x:.1f} t')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "bestmodeleval.train(trainloader, testloader, validation=False)\n",
    "accuracy_test = bestmodeleval.test(testloader)\n",
    "print('Accuracy of best model on test set with lr= {0:.2f}, hidden units= {1:.2f}, is {2:.2f}'.format(best_lr, best_n_hidden, accuracy_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
