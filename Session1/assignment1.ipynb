{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Vision Lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Two layer neural network using numpy\n",
    "\n",
    "\n",
    "In this task we implement a simple neural network (NN) with an imput and an output layer. We use MNIST for our experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "import os, struct\n",
    "from array import array as pyarray\n",
    "from numpy import  array, zeros\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to load mnist dataset\n",
    "\n",
    "A simple function to load mnist dataset. Note: This function expects data files in a path $\\textit{'data/raw/'}$ from a current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(datafile='', labelfile=''):\n",
    "    \"\"\"\n",
    "    Loads MNIST files into 3D numpy arrays \n",
    "    Adapted from: http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "    \"\"\"\n",
    "    DATA_PATH = 'data/raw/'\n",
    "    images = open(DATA_PATH + datafile, 'rb')\n",
    "    flabel = open(DATA_PATH + labelfile, 'rb')\n",
    "    magic_nr, size = struct.unpack(\">II\", flabel.read(8))\n",
    "    label = pyarray(\"b\", flabel.read())\n",
    "    flabel.close()\n",
    "\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", images.read(16))\n",
    "    img = pyarray(\"B\", images.read())\n",
    "    images.close()\n",
    "\n",
    "    ind = [k for k in range(size) if label[k] in np.arange(10)]\n",
    "    N = len(ind)\n",
    "\n",
    "    images = zeros((N, rows, cols))\n",
    "    labels = zeros((N ) )\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = label[ind[i]]\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n",
    "We implement logistic regression for multi class classification as a 2 layer neural network. For this task we have implemented a class $\\textbf{SimpleNN}$. We use softmax as a non-linearity and mean square error as a loss function. We encountered a problem of stability with softmax. To address the problem we use clipping which prevents exponent scores from divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "\n",
    "    def __init__(self, n_in, n_out, alpha, batch_size, nm_iters, nonlin='softmax'):\n",
    "        # Initializa various parameters for a NN\n",
    "        self.n_in = n_in # Number of Input Neuron\n",
    "        self.n_out = n_out # Number of Output Neuron\n",
    "        self.alpha = alpha # Learning Rate\n",
    "        self.batch_size = batch_size # Batch Size\n",
    "        self.nm_iters = nm_iters # Number of iterations\n",
    "        self.nonlin = nonlin # Type of non linearity\n",
    "        self.W = np.random.randn(self.n_in, self.n_out) #Weight initialization\n",
    "        self.bias = np.zeros(self.n_out) #Bias Initialization\n",
    "        \n",
    "    # Implement softmax as a non-linearity\n",
    "    def softmax(self, x):\n",
    "        # Clipping to ensure stability\n",
    "        exp_scores = np.exp(x-np.max(x))\n",
    "        prob = (exp_scores.T / np.sum(exp_scores, axis=1)).T\n",
    "        return prob\n",
    "    \n",
    "    def MSElossfunc(self, y, pred):\n",
    "        loss = np.sum(np.power((y-pred),2), axis=1)\n",
    "        return np.sum(loss)\n",
    "    \n",
    "    # forward pass to compute probability scores\n",
    "    def forward(self, X):\n",
    "        # Forward Pass \n",
    "        pred = np.dot(X,self.W) + self.bias\n",
    "        return self.softmax(pred)\n",
    "    \n",
    "    # for batchwise training, given function generates minibatches of data\n",
    "    def get_minibatches(self, X, y, shuffle=True):\n",
    "        minibatches = []\n",
    "        X_shuff, y_shuff = np.copy(X), np.copy(y)\n",
    "        if shuffle:\n",
    "            X_shuff, y_shuff = skshuffle(X_shuff, y_shuff)\n",
    "        for i in range(0, X_shuff.shape[0], self.batch_size):\n",
    "            yield X_shuff[i:i + self.batch_size], y_shuff[i:i+self.batch_size]\n",
    "    \n",
    "    # batchwise training function\n",
    "    def train(self, X, y):\n",
    "        loss_iter = []\n",
    "        for i in range(self.nm_iters):\n",
    "            # Generate Mini-batches\n",
    "            mb_iter = self.get_minibatches(X, y, shuffle=True)\n",
    "            loss_mb = []\n",
    "            for X_mb, y_mb in mb_iter:\n",
    "                pred_mb = self.forward(X_mb)\n",
    "                loss = self.MSElossfunc(y_mb, pred_mb)\n",
    "                # Compute Gradients\n",
    "                grad = np.copy(pred_mb)\n",
    "                grad[range(self.batch_size),y_mb.argmax(axis=1)] -= 1\n",
    "                gradL_W = 2*np.dot(X_mb.T, grad)\n",
    "                gradL_b = 2*np.sum(grad, axis=0, keepdims=True).flatten()\n",
    "                # Gradient Descent\n",
    "                self.W -= self.alpha*gradL_W\n",
    "                self.bias -= self.alpha*gradL_b\n",
    "                loss_mb.append(loss)\n",
    "            loss_ = np.mean(loss_mb)\n",
    "            print(loss_)\n",
    "            loss_iter.append(loss_)\n",
    "        return loss_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utilities to visualize the loss and report the performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(loss):\n",
    "    plt.plot(range(len(loss)), loss)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()  \n",
    "\n",
    "def accuracy(pred_label, y_test):\n",
    "    accuracy = sum(pred_label == y_test)/(float(len(y_test)))\n",
    "    print('Accuracy {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent divergence of loss we scale all images by the factor of 255 (maximum pixel value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist(datafile='train-images-idx3-ubyte', labelfile='train-labels-idx1-ubyte')\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "# Encode label as one hot vector\n",
    "y_train = y_train.reshape(1, y_train.shape[0])\n",
    "y_train = np.eye(10)[y_train.astype('int32')]\n",
    "y_train = y_train.reshape(y_train.shape[1], y_train.shape[2])\n",
    "\n",
    "X_train = X_train /255\n",
    "n_in = X_train.shape[1]\n",
    "n_out = 10\n",
    "\n",
    "alpha = 0.00001\n",
    "batch_size = 200\n",
    "nm_iters = 200\n",
    "nonlin = 'softmax'\n",
    "nn_model = SimpleNN(n_in, n_out, alpha, batch_size, nm_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.012410182\n",
      "285.432184992\n",
      "269.440646294\n",
      "254.522044862\n",
      "238.917571875\n",
      "223.260123916\n",
      "208.868816412\n",
      "196.241897898\n",
      "185.410426447\n",
      "176.051812748\n",
      "167.841436648\n",
      "160.716414874\n",
      "154.327172134\n",
      "148.594213971\n",
      "143.524484795\n",
      "138.924965861\n",
      "134.712936626\n",
      "130.879624546\n",
      "127.416051245\n",
      "124.219872247\n",
      "121.337950097\n",
      "118.634845844\n",
      "116.10424539\n",
      "113.773555406\n",
      "111.638831368\n",
      "109.612738828\n",
      "107.730852566\n",
      "105.945625372\n",
      "104.235132952\n",
      "102.653172228\n",
      "101.150246218\n",
      "99.7211373742\n",
      "98.3716087048\n",
      "97.0556932677\n",
      "95.8385022838\n",
      "94.643944925\n",
      "93.5101030541\n",
      "92.4365441889\n",
      "91.3869231251\n",
      "90.3893694161\n",
      "89.419015682\n",
      "88.4857063037\n",
      "87.6220115022\n",
      "86.7449314973\n",
      "85.9140523965\n",
      "85.118530876\n",
      "84.3436008611\n",
      "83.5973619795\n",
      "82.8651232796\n",
      "82.165185271\n",
      "81.4831578703\n",
      "80.8269101027\n",
      "80.1802346765\n",
      "79.5642246802\n",
      "78.9506871054\n",
      "78.3852376296\n",
      "77.7979250344\n",
      "77.2544705417\n",
      "76.7075915349\n",
      "76.1935161828\n",
      "75.672169057\n",
      "75.1817432382\n",
      "74.6972411061\n",
      "74.2182208317\n",
      "73.7592613898\n",
      "73.3018394009\n",
      "72.8697144951\n",
      "72.4351608133\n",
      "72.0112759908\n",
      "71.6025526141\n",
      "71.1969716657\n",
      "70.8017515614\n",
      "70.4113013565\n",
      "70.0353078306\n",
      "69.6585568787\n",
      "69.2990199456\n",
      "68.9428451677\n",
      "68.5931567563\n",
      "68.2405332947\n",
      "67.9126617054\n",
      "67.5824411829\n",
      "67.2554847837\n",
      "66.9388791029\n",
      "66.627602472\n",
      "66.3191766334\n",
      "66.0156162129\n",
      "65.7143009104\n",
      "65.4367581501\n",
      "65.1425940397\n",
      "64.8621877473\n",
      "64.5885217471\n",
      "64.3194181376\n",
      "64.0468198468\n",
      "63.7892566474\n",
      "63.5256649405\n",
      "63.2752383863\n",
      "63.030240053\n",
      "62.7865965478\n",
      "62.540148928\n",
      "62.3012748232\n",
      "62.0709767536\n",
      "61.8404684576\n",
      "61.6173570285\n",
      "61.3903499656\n",
      "61.1674198349\n",
      "60.9554648351\n",
      "60.7402013465\n",
      "60.536136262\n",
      "60.325602517\n",
      "60.1241796198\n",
      "59.9292689169\n",
      "59.7284612191\n",
      "59.5375992521\n",
      "59.3431866723\n",
      "59.1593688627\n",
      "58.9670229484\n",
      "58.7882265128\n",
      "58.5998458482\n",
      "58.4301143657\n",
      "58.2511239141\n",
      "58.0858405592\n",
      "57.9116466707\n",
      "57.7310912395\n",
      "57.5713660865\n",
      "57.4017395302\n",
      "57.2488512962\n",
      "57.0880188258\n",
      "56.9298890445\n",
      "56.7715740752\n",
      "56.6174750448\n",
      "56.4706112037\n",
      "56.3125787048\n",
      "56.1688208168\n",
      "56.0246285461\n",
      "55.8816579465\n",
      "55.7387155841\n",
      "55.5949140072\n",
      "55.4618369672\n",
      "55.3191896112\n",
      "55.1863023219\n",
      "55.0468770507\n",
      "54.9166940427\n",
      "54.7847406661\n",
      "54.6561340801\n",
      "54.521123199\n",
      "54.4055020513\n",
      "54.2775509131\n",
      "54.1512839876\n",
      "54.0270344727\n",
      "53.9010674318\n",
      "53.7805804913\n",
      "53.6693598746\n",
      "53.544469275\n",
      "53.428689431\n",
      "53.3165968644\n",
      "53.2072404993\n",
      "53.0814584464\n",
      "52.977969911\n",
      "52.8573007479\n",
      "52.7517889327\n",
      "52.6471131151\n",
      "52.5311311593\n",
      "52.4313849191\n",
      "52.3310776632\n",
      "52.2181973775\n",
      "52.1123274383\n",
      "52.0070527147\n",
      "51.9092499567\n",
      "51.8164317421\n",
      "51.7082532504\n",
      "51.6067656228\n",
      "51.5127431199\n",
      "51.4214471213\n",
      "51.3187085468\n",
      "51.2226892433\n",
      "51.1276147301\n",
      "51.0312203434\n",
      "50.9384478455\n",
      "50.8512200965\n",
      "50.7611304958\n",
      "50.6646868197\n",
      "50.580286984\n",
      "50.4872576544\n",
      "50.4031798937\n",
      "50.310596684\n",
      "50.2257412663\n",
      "50.1373582012\n",
      "50.0517992353\n",
      "49.9661526352\n",
      "49.8807442101\n",
      "49.8042483675\n",
      "49.7198955695\n",
      "49.6278898217\n",
      "49.5578621785\n",
      "49.4703871995\n",
      "49.3939809748\n",
      "49.3126478903\n",
      "49.2273437191\n",
      "49.1565233454\n",
      "49.0837948109\n"
     ]
    }
   ],
   "source": [
    "loss = nn_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHxttZpaW7FHsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30f1877400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist(datafile='t10k-images-idx3-ubyte', labelfile='t10k-labels-idx1-ubyte')\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_test = X_test /255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.85\n"
     ]
    }
   ],
   "source": [
    "pred_test = nn_model.forward(X_test)\n",
    "pred_label = np.argmax(pred_test, axis=1)\n",
    "accuracy(pred_label, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.91      0.94      0.93       980\n",
      "        1.0       0.94      0.95      0.94      1135\n",
      "        2.0       0.86      0.83      0.84      1032\n",
      "        3.0       0.83      0.85      0.84      1010\n",
      "        4.0       0.84      0.85      0.85       982\n",
      "        5.0       0.78      0.76      0.77       892\n",
      "        6.0       0.90      0.88      0.89       958\n",
      "        7.0       0.87      0.87      0.87      1028\n",
      "        8.0       0.77      0.79      0.78       974\n",
      "        9.0       0.81      0.79      0.80      1009\n",
      "\n",
      "avg / total       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
